{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def velocities_from_boxes(boxes_prev, boxes):\n",
    "    \"\"\"Computes bounding box velocities.\n",
    "    \n",
    "    Args:\n",
    "        boxes_prev (`torch.Tensor`): Bounding boxes in previous frame. Shape [B, N, 4] or \n",
    "            [N, 4] where B is the batch size and N the number of bounding boxes.\n",
    "            \n",
    "        boxes (`torch.Tensor`): Bounding boxes in current frame. Shape [B, N, 4] or [N, 4] \n",
    "            where B is the batch size and N the number of bounding boxes.\n",
    "            \n",
    "    Returns:\n",
    "        (`torch.Tensor`) velocities of box coordinates in current frame. Shape [B, N, 4] \n",
    "        or [N, 4] depending on whether batch dimension is used or not.\n",
    "        \n",
    "    Ensure that ordering of boxes in both tensors is consistent and that the number of boxes\n",
    "    is the same.\n",
    "    \"\"\"\n",
    "    x = boxes[..., 0].unsqueeze(-1)\n",
    "    y = boxes[..., 1].unsqueeze(-1)\n",
    "    w = boxes[..., 2].unsqueeze(-1)\n",
    "    h = boxes[..., 3].unsqueeze(-1) \n",
    "    x_p = boxes_prev[..., 0].unsqueeze(-1)\n",
    "    y_p = boxes_prev[..., 1].unsqueeze(-1)\n",
    "    w_p = boxes_prev[..., 2].unsqueeze(-1)\n",
    "    h_p = boxes_prev[..., 3].unsqueeze(-1)    \n",
    "    v_x = 1 / w_p * (x - x_p)\n",
    "    v_y = 1 / h_p * (y - y_p)\n",
    "    v_w = torch.log(w / w_p)\n",
    "    v_h = torch.log(h / h_p)    \n",
    "    return torch.cat([v_x, v_y, v_w, v_h], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_from_velocities(boxes_prev, velocities):\n",
    "    \"\"\"Computes bounding boxes from previous boxes and velocities.\n",
    "    \n",
    "    Args:\n",
    "        boxes_prev (`torch.Tensor`): Bounding boxes in previous frame. Shape [B, N, 4] \n",
    "            where B is the batch size and N the number of bounding boxes.\n",
    "            \n",
    "        velocities (`torch.Tensor`): Box velocities in current frame. Shape [B, N, 4] \n",
    "        where B is the batch size and N the number of bounding boxes.\n",
    "            \n",
    "    Returns:\n",
    "        (`torch.Tensor`) Bounding boxes in current frame. Shape [B, N, 4].\n",
    "        \n",
    "    Ensure that ordering of boxes and velocities in both tensors is consistent that is\n",
    "    box in row i should correspond to velocities in row i.\n",
    "    \"\"\"\n",
    "    x_p = boxes_prev[..., 0].unsqueeze(-1)\n",
    "    y_p = boxes_prev[..., 1].unsqueeze(-1)\n",
    "    w_p = boxes_prev[..., 2].unsqueeze(-1)\n",
    "    h_p = boxes_prev[..., 3].unsqueeze(-1)\n",
    "    v_x = velocities[..., 0].unsqueeze(-1)\n",
    "    v_y = velocities[..., 1].unsqueeze(-1)\n",
    "    v_w = velocities[..., 2].unsqueeze(-1)\n",
    "    v_h = velocities[..., 3].unsqueeze(-1)\n",
    "    x = w_p * v_x + x_p\n",
    "    y = h_p * v_y + y_p\n",
    "    w = w_p * torch.exp(v_w)\n",
    "    h = h_p * torch.exp(v_h)\n",
    "    return torch.cat([x, y, w, h], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotionVectorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, mode, keyframe_interval=10):\n",
    "        self.mode = mode\n",
    "        self.keyframe_interval = keyframe_interval\n",
    "        data_file = os.path.join(root_dir, \"preprocessed\", mode, \"data.pkl\")\n",
    "        self.data = pickle.load(open(data_file, \"rb\"))\n",
    "               \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.mode == \"train\" or self.mode == \"val\":\n",
    "            \n",
    "            motion_vectors = self.data[idx][\"motion_vectors\"]\n",
    "            \n",
    "            gt_ids = self.data[idx][\"gt_ids\"]\n",
    "            gt_boxes = self.data[idx][\"gt_boxes\"]\n",
    "            gt_ids_prev = self.data[idx][\"gt_ids_prev\"]\n",
    "            gt_boxes_prev = self.data[idx][\"gt_boxes_prev\"]\n",
    "\n",
    "            # find boxes which occured in the last frame\n",
    "            _, idx_1, idx_0 = np.intersect1d(gt_ids, gt_ids_prev, assume_unique=True, return_indices=True)\n",
    "            boxes = torch.from_numpy(gt_boxes[idx_1]).unsqueeze(0)\n",
    "            boxes_prev = torch.from_numpy(gt_boxes_prev[idx_0]).unsqueeze(0)\n",
    "            velocities = velocities_from_boxes(boxes_prev, boxes)\n",
    "            \n",
    "            return motion_vectors.float(), boxes_prev.float(), velocities.float()\n",
    "        \n",
    "        # TODO: handle test case"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "each sample must contain\n",
    "\n",
    "train/val:\n",
    "\n",
    "x: MV_t, B_{t-1}\n",
    "y: v_t\n",
    "\n",
    "boxes must be ordered consistently since last keyframe, only boxes with same gt_ids must be considered\n",
    "\n",
    "test:\n",
    "\n",
    "x: MV_t, Bd_{0} where t = 0 is the last keyframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagation Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropagationNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PropagationNetwork, self).__init__()\n",
    "        self.base = torchvision.models.resnet18(pretrained=True)\n",
    "        \n",
    "        # change number of input channels from 3 to 2\n",
    "        #self.base.conv1.in_channels = 2\n",
    "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "         \n",
    "        # remove fully connected and avg pool layers\n",
    "        self.base = nn.Sequential(*list(self.base.children())[:-2])\n",
    "        \n",
    "        # change stride to 1 in conv5 block\n",
    "        #self.base[5][0].conv1 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), dilation=2, padding=(1, 1), bias=False)\n",
    "        #self.base[5][0].conv2 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), dilation=2, padding=(1, 1), bias=False)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(512, 196, kernel_size=(1, 1), stride=(1, 1))\n",
    "        \n",
    "        #print(list(self.base.children())[5][0].conv1)\n",
    "        #print(list(self.children()))\n",
    "        \n",
    "    def forward(self, motion_vectors, boxes_prev):\n",
    "        boxes_prev = self._change_box_format(boxes_prev)\n",
    "        print(\"boxes_prev:\", boxes_prev.shape)\n",
    "        print(\"motion_vectors:\", motion_vectors.shape)\n",
    "        x = self.base(motion_vectors)\n",
    "        print(\"after ResNet18:\", x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        print(\"after conv1\", x.shape)\n",
    "        velocities_pred = ops.roi_pool(x, boxes_prev, output_size=(7, 7))\n",
    "        print(\"after roi_pool\", velocities_pred.shape)\n",
    "        \n",
    "        # velocities_pred should have shape [B, C, I, 4]\n",
    "        \n",
    "        return velocities_pred\n",
    "    \n",
    "    \n",
    "    def _change_box_format(self, boxes):\n",
    "        \"\"\"Change format of boxes from [x, y, w, h] to [x1, y1, x2, y2].\"\"\"        \n",
    "        boxes[..., 0] = boxes[..., 0]\n",
    "        boxes[..., 1] = boxes[..., 1]\n",
    "        boxes[..., 2] = boxes[..., 0] + boxes[..., 2]\n",
    "        boxes[..., 3] = boxes[..., 1] + boxes[..., 3]\n",
    "        return boxes\n",
    "    \n",
    "#model = PropagationNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, num_epochs=2):\n",
    "    tstart = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    pickle.dump(best_model_wts, open(\"models/best_model.pkl\", \"wb\"))\n",
    "    best_loss = 999999.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs-1))\n",
    "        \n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "\n",
    "            for motion_vectors, boxes_prev, velocities in dataloaders[phase]:\n",
    "                motion_vectors = motion_vectors.to(device)\n",
    "                boxes_prev = boxes_prev.to(device)\n",
    "                velocities = velocities.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    velocities_pred = model(motion_vectors, boxes_prev)\n",
    "                    loss = criterion(velocities_pred, velocities)\n",
    "                    \n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        \n",
    "                running_loss += loss.item() * x.size(0)\n",
    "                \n",
    "            epoch_loss = running_loss / len(datasets[phase])\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            \n",
    "            if phase == \"val\":\n",
    "                model_wts = copy.deepcopy(model.state_dict())\n",
    "                pickle.dump(best_model_wts, open(\"models/model_{:04d}.pkl\".format(epoch), \"wb\"))\n",
    "            \n",
    "            if phase == \"val\" and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                pickle.dump(best_model_wts, open(\"models/best_model.pkl\", \"wb\"))\n",
    "                \n",
    "    time_elapsed = time.time() - tstart\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Lowest validation loss: {:4f}'.format(best_loss))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "boxes_prev: torch.Size([8, 1, 22, 4])\n",
      "motion_vectors: torch.Size([8, 2, 68, 120])\n",
      "after ResNet18: torch.Size([8, 512, 3, 4])\n",
      "after conv1 torch.Size([8, 196, 3, 4])\n",
      "after roi_pool torch.Size([8, 196, 7, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:782: UserWarning: Using a target size (torch.Size([8, 1, 22, 4])) that is different to the input size (torch.Size([8, 196, 7, 7])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (7) must match the size of tensor b (4) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-285-d5f174d119d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-284-0fa4bd287660>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mvelocities_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmotion_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvelocities_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvelocities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msmooth_l1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 [0, 1, 2]])\n\u001b[1;32m     52\u001b[0m     \"\"\"\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (7) must match the size of tensor b (4) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "datasets = {x: MotionVectorDataset(root_dir='../benchmark/MOT17', mode=x) for x in [\"train\", \"val\", \"test\"]}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=8, shuffle=False) for x in [\"train\", \"val\", \"test\"]}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PropagationNetwork()\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.SmoothL1Loss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)\n",
    "train(model, criterion, optimizer, scheduler, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataloaders[\"train\"]))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# test cases for utility functions\n",
    "gt_boxes_prev = np.array([[1338.,  418.,  167.,  379.],\n",
    "          [ 586.,  447.,   85.,  263.],\n",
    "          [1416.,  431.,  184.,  336.],\n",
    "          [1056.,  484.,   36.,  110.],\n",
    "          [1091.,  484.,   31.,  115.],\n",
    "          [1255.,  447.,   33.,  100.],\n",
    "          [1016.,  430.,   40.,  116.],\n",
    "          [1101.,  441.,   38.,  108.],\n",
    "          [ 935.,  436.,   42.,  114.],\n",
    "          [ 442.,  446.,  105.,  283.],\n",
    "          [ 636.,  458.,   61.,  187.],\n",
    "          [1364.,  434.,   51.,  124.],\n",
    "          [1478.,  434.,   63.,  124.],\n",
    "          [ 473.,  460.,   89.,  249.],\n",
    "          [ 548.,  465.,   35.,   93.],\n",
    "          [ 418.,  459.,   40.,   84.],\n",
    "          [ 582.,  456.,   35.,  133.],\n",
    "          [ 972.,  456.,   32.,   77.],\n",
    "          [ 578.,  432.,   20.,   43.],\n",
    "          [ 596.,  429.,   18.,   42.],\n",
    "          [ 663.,  451.,   34.,   86.]])\n",
    "\n",
    "gt_boxes = np.array([[1342.,  417.,  168.,  380.],\n",
    "          [ 586.,  446.,   85.,  264.],\n",
    "          [1422.,  431.,  183.,  337.],\n",
    "          [1055.,  483.,   36.,  110.],\n",
    "          [1090.,  484.,   32.,  114.],\n",
    "          [1255.,  447.,   33.,  100.],\n",
    "          [1015.,  430.,   40.,  116.],\n",
    "          [1100.,  440.,   38.,  108.],\n",
    "          [ 934.,  435.,   42.,  114.],\n",
    "          [ 442.,  446.,  107.,  282.],\n",
    "          [ 636.,  458.,   61.,  187.],\n",
    "          [1365.,  434.,   52.,  124.],\n",
    "          [1480.,  433.,   62.,  125.],\n",
    "          [ 473.,  460.,   89.,  249.],\n",
    "          [ 547.,  464.,   35.,   93.],\n",
    "          [ 418.,  459.,   40.,   84.],\n",
    "          [ 582.,  455.,   34.,  134.],\n",
    "          [ 972.,  456.,   32.,   77.],\n",
    "          [ 578.,  431.,   20.,   43.],\n",
    "          [ 595.,  428.,   18.,   42.],\n",
    "          [1035.,  452.,   25.,   67.],\n",
    "          [ 664.,  451.,   34.,   85.]])\n",
    "\n",
    "gt_ids = np.array([ 2.,  3.,  8.,  9., 10., 14., 15., 17., 18., 19., 20., 21., 22., 23.,\n",
    "          26., 31., 36., 39., 68., 69., 70., 72.])\n",
    "\n",
    "gt_ids_prev = np.array([ 2.,  3.,  8.,  9., 10., 14., 15., 17., 18., 19., 20., 21., 22., 23.,\n",
    "          26., 31., 36., 39., 68., 69., 72.])\n",
    "\n",
    "_, idx_1, idx_0 = np.intersect1d(gt_ids, gt_ids_prev, assume_unique=True, return_indices=True)\n",
    "print(idx_1, idx_0)\n",
    "boxes = torch.from_numpy(gt_boxes[idx_1]).unsqueeze(0)\n",
    "boxes_prev = torch.from_numpy(gt_boxes_prev[idx_0]).unsqueeze(0)\n",
    "print(boxes.shape)\n",
    "print(boxes_prev.shape)\n",
    "velocities = velocity_from_boxes(boxes_prev, boxes)\n",
    "print(velocities)\n",
    "\n",
    "\n",
    "\n",
    "# TODO: inverse function\n",
    "\n",
    "\n",
    "box = box_from_velocities(boxes_prev, velocities)\n",
    "print(box)\n",
    "print(boxes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mvs_x = (item[\"x\"].numpy())[:, :, 0]\n",
    "mvs_y = (item[\"x\"].numpy())[:, :, 1]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(25,15))\n",
    "ax.quiver(xi, yi, mvs_x, mvs_y, scale=1000, color='r')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input:\n",
    "    - motion vectors at frame t-w, t-w+1, t-w+2,... t-1, t (window length w)\n",
    "    - bounding box coordinates (x, y, w, h) at frame t-w, t-w+1, t-w+2,... t-1\n",
    "    \n",
    "Output:\n",
    "    - bounding box coordinates for frame t\n",
    "    \n",
    "Difficulties:\n",
    "    - keyframes do not have any MVs\n",
    "    - number of bounding boxes in frame can vary over time\n",
    "    - number of vectors inside bounding box can vary over time\n",
    "    - size of bounding box changes over time\n",
    "    \n",
    "Ideas:\n",
    "    1) Encode each vector as magnitude and direction\n",
    "    2) Extrapolate vector values on evenly spaced 16 x 16 grid\n",
    "    3) Use CNN-LSTM to encode the sequence of vectors and use attention to guide to bounding box region only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
