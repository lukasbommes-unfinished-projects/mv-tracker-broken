{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def velocities_from_boxes(boxes_prev, boxes):\n",
    "    \"\"\"Computes bounding box velocities.\n",
    "    \n",
    "    Args:\n",
    "        boxes_prev (`torch.Tensor`): Bounding boxes in previous frame. Shape [B, N, 4]\n",
    "            where B is the batch size and N the number of bounding boxes.\n",
    "            \n",
    "        boxes (`torch.Tensor`): Bounding boxes in current frame. Shape [B, N, 4]\n",
    "            where B is the batch size and N the number of bounding boxes.\n",
    "            \n",
    "    Returns:\n",
    "        (`torch.Tensor`) velocities of box coordinates in current frame. Shape [B, N, 4].\n",
    "        \n",
    "    Ensure that ordering of boxes in both tensors is consistent and that the number of boxes\n",
    "    is the same.\n",
    "    \"\"\"\n",
    "    x = boxes[:, 0]\n",
    "    y = boxes[:, 1]\n",
    "    w = boxes[:, 2]\n",
    "    h = boxes[:, 3]\n",
    "    x_p = boxes_prev[:, 0]\n",
    "    y_p = boxes_prev[:, 1]\n",
    "    w_p = boxes_prev[:, 2]\n",
    "    h_p = boxes_prev[:, 3] \n",
    "    v_x = (1 / w_p * (x - x_p)).unsqueeze(-1)\n",
    "    v_y = (1 / h_p * (y - y_p)).unsqueeze(-1)\n",
    "    v_w = (torch.log(w / w_p)).unsqueeze(-1)\n",
    "    v_h = (torch.log(h / h_p)).unsqueeze(-1) \n",
    "    return torch.cat([v_x, v_y, v_w, v_h], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_from_velocities(boxes_prev, velocities):\n",
    "    \"\"\"Computes bounding boxes from previous boxes and velocities.\n",
    "    \n",
    "    Args:\n",
    "        boxes_prev (`torch.Tensor`): Bounding boxes in previous frame. Shape [B, N, 4] \n",
    "            where B is the batch size and N the number of bounding boxes.\n",
    "            \n",
    "        velocities (`torch.Tensor`): Box velocities in current frame. Shape [B, N, 4] \n",
    "        where B is the batch size and N the number of bounding boxes.\n",
    "            \n",
    "    Returns:\n",
    "        (`torch.Tensor`) Bounding boxes in current frame. Shape [B, N, 4].\n",
    "        \n",
    "    Ensure that ordering of boxes and velocities in both tensors is consistent that is\n",
    "    box in row i should correspond to velocities in row i.\n",
    "    \"\"\"\n",
    "    x_p = boxes_prev[:, 0]\n",
    "    y_p = boxes_prev[:, 1]\n",
    "    w_p = boxes_prev[:, 2]\n",
    "    h_p = boxes_prev[:, 3]\n",
    "    v_x = velocities[:, 0]\n",
    "    v_y = velocities[:, 1]\n",
    "    v_w = velocities[:, 2]\n",
    "    v_h = velocities[:, 3]\n",
    "    x = (w_p * v_x + x_p).unsqueeze(-1)\n",
    "    y = (h_p * v_y + y_p).unsqueeze(-1)\n",
    "    w = (w_p * torch.exp(v_w)).unsqueeze(-1)\n",
    "    h = (h_p * torch.exp(v_h)).unsqueeze(-1)\n",
    "    return torch.cat([x, y, w, h], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([2, 1080, 1920, 3])\n",
      "torch.Size([2, 2, 68, 120])\n",
      "torch.Size([2, 52, 5])\n",
      "torch.Size([2, 52, 4])\n",
      "torch.Size([2, 52, 4])\n",
      "torch.Size([2, 52])\n",
      "torch.Size([2, 52])\n",
      "torch.Size([2, 52])\n",
      "tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.5000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.5000,  3.5000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.5000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, -0.5000,  ...,  0.0000,  0.0000, -0.5000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.5000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -1.5000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ..., -0.5000,  0.5000,  0.5000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]])\n",
      "tensor([[1.0000e+00, 1.3380e+03, 4.1800e+02, 1.6700e+02, 3.7900e+02],\n",
      "        [1.0000e+00, 5.8600e+02, 4.4700e+02, 8.5000e+01, 2.6300e+02],\n",
      "        [1.0000e+00, 1.4160e+03, 4.3100e+02, 1.8400e+02, 3.3600e+02],\n",
      "        [1.0000e+00, 1.0560e+03, 4.8400e+02, 3.6000e+01, 1.1000e+02],\n",
      "        [1.0000e+00, 1.0910e+03, 4.8400e+02, 3.1000e+01, 1.1500e+02],\n",
      "        [1.0000e+00, 1.2550e+03, 4.4700e+02, 3.3000e+01, 1.0000e+02],\n",
      "        [1.0000e+00, 1.0160e+03, 4.3000e+02, 4.0000e+01, 1.1600e+02],\n",
      "        [1.0000e+00, 1.1010e+03, 4.4100e+02, 3.8000e+01, 1.0800e+02],\n",
      "        [1.0000e+00, 9.3500e+02, 4.3600e+02, 4.2000e+01, 1.1400e+02],\n",
      "        [1.0000e+00, 4.4200e+02, 4.4600e+02, 1.0500e+02, 2.8300e+02],\n",
      "        [1.0000e+00, 6.3600e+02, 4.5800e+02, 6.1000e+01, 1.8700e+02],\n",
      "        [1.0000e+00, 1.3640e+03, 4.3400e+02, 5.1000e+01, 1.2400e+02],\n",
      "        [1.0000e+00, 1.4780e+03, 4.3400e+02, 6.3000e+01, 1.2400e+02],\n",
      "        [1.0000e+00, 4.7300e+02, 4.6000e+02, 8.9000e+01, 2.4900e+02],\n",
      "        [1.0000e+00, 5.4800e+02, 4.6500e+02, 3.5000e+01, 9.3000e+01],\n",
      "        [1.0000e+00, 4.1800e+02, 4.5900e+02, 4.0000e+01, 8.4000e+01],\n",
      "        [1.0000e+00, 5.8200e+02, 4.5600e+02, 3.5000e+01, 1.3300e+02],\n",
      "        [1.0000e+00, 9.7200e+02, 4.5600e+02, 3.2000e+01, 7.7000e+01],\n",
      "        [1.0000e+00, 5.7800e+02, 4.3200e+02, 2.0000e+01, 4.3000e+01],\n",
      "        [1.0000e+00, 5.9600e+02, 4.2900e+02, 1.8000e+01, 4.2000e+01],\n",
      "        [1.0000e+00, 1.0360e+03, 4.5300e+02, 2.5000e+01, 6.7000e+01],\n",
      "        [1.0000e+00, 6.6300e+02, 4.5100e+02, 3.4000e+01, 8.6000e+01],\n",
      "        [2.0000e+00, 1.3420e+03, 4.1700e+02, 1.6800e+02, 3.8000e+02],\n",
      "        [2.0000e+00, 5.8600e+02, 4.4600e+02, 8.5000e+01, 2.6400e+02],\n",
      "        [2.0000e+00, 1.4220e+03, 4.3100e+02, 1.8300e+02, 3.3700e+02],\n",
      "        [2.0000e+00, 1.0550e+03, 4.8300e+02, 3.6000e+01, 1.1000e+02],\n",
      "        [2.0000e+00, 1.0900e+03, 4.8400e+02, 3.2000e+01, 1.1400e+02],\n",
      "        [2.0000e+00, 1.2550e+03, 4.4700e+02, 3.3000e+01, 1.0000e+02],\n",
      "        [2.0000e+00, 1.0150e+03, 4.3000e+02, 4.0000e+01, 1.1600e+02],\n",
      "        [2.0000e+00, 1.1000e+03, 4.4000e+02, 3.8000e+01, 1.0800e+02],\n",
      "        [2.0000e+00, 9.3400e+02, 4.3500e+02, 4.2000e+01, 1.1400e+02],\n",
      "        [2.0000e+00, 4.4200e+02, 4.4600e+02, 1.0700e+02, 2.8200e+02],\n",
      "        [2.0000e+00, 6.3600e+02, 4.5800e+02, 6.1000e+01, 1.8700e+02],\n",
      "        [2.0000e+00, 1.3650e+03, 4.3400e+02, 5.2000e+01, 1.2400e+02],\n",
      "        [2.0000e+00, 1.4800e+03, 4.3300e+02, 6.2000e+01, 1.2500e+02],\n",
      "        [2.0000e+00, 4.7300e+02, 4.6000e+02, 8.9000e+01, 2.4900e+02],\n",
      "        [2.0000e+00, 5.4700e+02, 4.6400e+02, 3.5000e+01, 9.3000e+01],\n",
      "        [2.0000e+00, 4.1800e+02, 4.5900e+02, 4.0000e+01, 8.4000e+01],\n",
      "        [2.0000e+00, 5.8200e+02, 4.5500e+02, 3.4000e+01, 1.3400e+02],\n",
      "        [2.0000e+00, 9.7200e+02, 4.5600e+02, 3.2000e+01, 7.7000e+01],\n",
      "        [2.0000e+00, 5.7800e+02, 4.3100e+02, 2.0000e+01, 4.3000e+01],\n",
      "        [2.0000e+00, 5.9500e+02, 4.2800e+02, 1.8000e+01, 4.2000e+01],\n",
      "        [2.0000e+00, 1.0350e+03, 4.5200e+02, 2.5000e+01, 6.7000e+01],\n",
      "        [2.0000e+00, 6.6400e+02, 4.5100e+02, 3.4000e+01, 8.5000e+01]])\n",
      "tensor([[1342.,  417.,  168.,  380.],\n",
      "        [ 586.,  446.,   85.,  264.],\n",
      "        [1422.,  431.,  183.,  337.],\n",
      "        [1055.,  483.,   36.,  110.],\n",
      "        [1090.,  484.,   32.,  114.],\n",
      "        [1255.,  447.,   33.,  100.],\n",
      "        [1015.,  430.,   40.,  116.],\n",
      "        [1100.,  440.,   38.,  108.],\n",
      "        [ 934.,  435.,   42.,  114.],\n",
      "        [ 442.,  446.,  107.,  282.],\n",
      "        [ 636.,  458.,   61.,  187.],\n",
      "        [1365.,  434.,   52.,  124.],\n",
      "        [1480.,  433.,   62.,  125.],\n",
      "        [ 473.,  460.,   89.,  249.],\n",
      "        [ 547.,  464.,   35.,   93.],\n",
      "        [ 418.,  459.,   40.,   84.],\n",
      "        [ 582.,  455.,   34.,  134.],\n",
      "        [ 972.,  456.,   32.,   77.],\n",
      "        [ 578.,  431.,   20.,   43.],\n",
      "        [ 595.,  428.,   18.,   42.],\n",
      "        [1035.,  452.,   25.,   67.],\n",
      "        [ 664.,  451.,   34.,   85.],\n",
      "        [1346.,  417.,  170.,  380.],\n",
      "        [ 586.,  446.,   85.,  264.],\n",
      "        [1428.,  431.,  182.,  338.],\n",
      "        [1055.,  483.,   36.,  110.],\n",
      "        [1090.,  484.,   32.,  114.],\n",
      "        [1255.,  447.,   33.,  100.],\n",
      "        [1015.,  430.,   40.,  116.],\n",
      "        [1100.,  440.,   38.,  108.],\n",
      "        [ 934.,  435.,   42.,  114.],\n",
      "        [ 442.,  446.,  109.,  282.],\n",
      "        [ 636.,  458.,   62.,  187.],\n",
      "        [1366.,  434.,   54.,  124.],\n",
      "        [1483.,  433.,   60.,  125.],\n",
      "        [ 474.,  460.,   89.,  249.],\n",
      "        [ 547.,  464.,   35.,   93.],\n",
      "        [ 418.,  459.,   40.,   84.],\n",
      "        [ 582.,  455.,   34.,  134.],\n",
      "        [ 973.,  456.,   32.,   77.],\n",
      "        [ 578.,  431.,   20.,   43.],\n",
      "        [ 595.,  428.,   18.,   42.],\n",
      "        [1035.,  452.,   25.,   67.],\n",
      "        [ 665.,  451.,   34.,   85.]])\n",
      "tensor([[ 0.0240, -0.0026,  0.0060,  0.0026],\n",
      "        [ 0.0000, -0.0038,  0.0000,  0.0038],\n",
      "        [ 0.0326,  0.0000, -0.0054,  0.0030],\n",
      "        [-0.0278, -0.0091,  0.0000,  0.0000],\n",
      "        [-0.0323,  0.0000,  0.0317, -0.0087],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0250,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0263, -0.0093,  0.0000,  0.0000],\n",
      "        [-0.0238, -0.0088,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0189, -0.0035],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0196,  0.0000,  0.0194,  0.0000],\n",
      "        [ 0.0317, -0.0081, -0.0160,  0.0080],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0286, -0.0108,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0075, -0.0290,  0.0075],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0233,  0.0000,  0.0000],\n",
      "        [-0.0556, -0.0238,  0.0000,  0.0000],\n",
      "        [-0.0400, -0.0149,  0.0000,  0.0000],\n",
      "        [ 0.0294,  0.0000,  0.0000, -0.0117],\n",
      "        [ 0.0238,  0.0000,  0.0118,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0328,  0.0000, -0.0055,  0.0030],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0185,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0163,  0.0000],\n",
      "        [ 0.0192,  0.0000,  0.0377,  0.0000],\n",
      "        [ 0.0484,  0.0000, -0.0328,  0.0000],\n",
      "        [ 0.0112,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0312,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0294,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[ 2.,  3.,  8.,  9., 10., 14., 15., 17., 18., 19., 20., 21., 22., 23.,\n",
      "         26., 31., 36., 39., 68., 69., 70., 72.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 2.,  3.,  8.,  9., 10., 14., 15., 17., 18., 19., 20., 21., 22., 23.,\n",
      "         26., 31., 36., 39., 68., 69., 70., 72.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "tensor([[ 2.,  3.,  8.,  9., 10., 14., 15., 17., 18., 19., 20., 21., 22., 23.,\n",
      "         26., 31., 36., 39., 68., 69., 70., 72.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 2.,  3.,  8.,  9., 10., 14., 15., 17., 18., 19., 20., 21., 22., 23.,\n",
      "         26., 31., 36., 39., 68., 69., 70., 72.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-30251d1823dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_ids_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquiver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmotion_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotion_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 4"
     ]
    }
   ],
   "source": [
    "class MotionVectorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, codec, mode, keyframe_interval=10, pad_num_boxes=52, window_length=3):\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        self.keyframe_interval = keyframe_interval\n",
    "        self.pad_num_boxes = pad_num_boxes\n",
    "        self.window_length = window_length\n",
    "        assert self.window_length > 0, \"window length must be 1 or greater\"\n",
    "        data_file = os.path.join(root_dir, \"preprocessed\", codec, mode, \"data.pkl\")\n",
    "        self.data = pickle.load(open(data_file, \"rb\"))\n",
    "        \n",
    "        # remove entries so that an integer number of sequences of window length can be generated\n",
    "        if self.window_length > 1:\n",
    "            lengths_file = os.path.join(root_dir, \"preprocessed\", codec, mode, \"lengths.pkl\")\n",
    "            lengths = pickle.load(open(lengths_file, \"rb\"))\n",
    "            lenghts_cumsum = np.cumsum(lengths)\n",
    "            for lenght, lenght_cumsum in zip(reversed(lengths), reversed(lenghts_cumsum)):\n",
    "                remainder = lenght % self.window_length  # e.g. 2\n",
    "                for r in range(remainder):\n",
    "                    self.data.pop(lenght_cumsum-1-r)\n",
    "                    \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data) - 1  # -1 is needed because of idx+1 in __getitem__\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.mode == \"train\" or self.mode == \"val\":\n",
    "            \n",
    "            frame_agg = []\n",
    "            motion_vectors_agg = []\n",
    "            boxes_agg = []\n",
    "            boxes_prev_agg = []\n",
    "            gt_ids_agg = []\n",
    "            gt_ids_prev_agg = []\n",
    "            velocities_agg = []\n",
    "            num_boxes_mask_agg = []\n",
    "            \n",
    "            # for every sample we expect 3 (window_length) entries\n",
    "            for ws in range(self.window_length):  # ws = 0, 1, 2\n",
    "                \n",
    "                frame_idx = self.data[idx + ws + 1][\"frame_idx\"]\n",
    "            \n",
    "                motion_vectors = self.data[idx + ws + 1][\"motion_vectors\"]\n",
    "                \n",
    "                # get current frame\n",
    "                sequence = self.data[idx + ws + 1][\"sequence\"]\n",
    "                frame_file = os.path.join(self.root_dir, \"train\", \"{}-FRCNN/img1/{:06d}.jpg\".format(sequence, frame_idx+1))\n",
    "                frame = torch.from_numpy(cv2.imread(frame_file, cv2.IMREAD_COLOR))\n",
    "\n",
    "                gt_ids = self.data[idx + ws + 1][\"gt_ids\"]\n",
    "                gt_boxes = self.data[idx + ws + 1][\"gt_boxes\"]\n",
    "                gt_ids_prev = self.data[idx + ws][\"gt_ids\"]\n",
    "                gt_boxes_prev = self.data[idx + ws][\"gt_boxes\"]\n",
    "\n",
    "                # find boxes which occured in the last frame\n",
    "                _, idx_1, idx_0 = np.intersect1d(gt_ids, gt_ids_prev, assume_unique=True, return_indices=True)\n",
    "                boxes = torch.from_numpy(gt_boxes[idx_1])\n",
    "                boxes_prev = torch.from_numpy(gt_boxes_prev[idx_0])\n",
    "                velocities = velocities_from_boxes(boxes_prev, boxes)\n",
    "                gt_ids = torch.from_numpy(gt_ids[idx_1])\n",
    "                gt_ids_prev = torch.from_numpy(gt_ids_prev[idx_0])\n",
    "\n",
    "                # insert frame index into boxes\n",
    "                num_boxes = (boxes.shape)[0]\n",
    "                boxes_prev_tmp = torch.zeros(num_boxes, 5)\n",
    "                boxes_prev_tmp[:, 1:5] = boxes_prev\n",
    "                boxes_prev_tmp[:, 0] = torch.full((num_boxes,), frame_idx)\n",
    "                boxes_prev = boxes_prev_tmp\n",
    "\n",
    "                # pad boxes_prev to the same global length (for MOT17 this is 52)\n",
    "                boxes_prev_padded = torch.zeros(self.pad_num_boxes, 5)\n",
    "                boxes_prev_padded[:num_boxes, :] = boxes_prev\n",
    "                boxes_prev = boxes_prev_padded\n",
    "                \n",
    "                # similarly pad boxes\n",
    "                boxes_padded = torch.zeros(self.pad_num_boxes, 4)\n",
    "                boxes_padded[:num_boxes, :] = boxes\n",
    "                boxes = boxes_padded\n",
    "\n",
    "                # similarly pad velocites\n",
    "                velocities_padded = torch.zeros(self.pad_num_boxes, 4)\n",
    "                velocities_padded[:num_boxes, :] = velocities\n",
    "                velocities = velocities_padded\n",
    "                \n",
    "                # similarly pad gt_ids\n",
    "                gt_ids_padded = torch.zeros(self.pad_num_boxes,)\n",
    "                gt_ids_padded[:num_boxes] = gt_ids\n",
    "                gt_ids = gt_ids_padded\n",
    "                gt_ids_prev_padded = torch.zeros(self.pad_num_boxes,)\n",
    "                gt_ids_prev_padded[:num_boxes] = gt_ids_prev\n",
    "                gt_ids_prev = gt_ids_prev_padded\n",
    "\n",
    "                # create a mask to revert the padding at a later stage\n",
    "                num_boxes_mask = torch.zeros(self.pad_num_boxes,)\n",
    "                num_boxes_mask[0:num_boxes] = torch.ones(num_boxes,)\n",
    "                \n",
    "                frame_agg.append(frame)\n",
    "                motion_vectors_agg.append(motion_vectors.float())\n",
    "                boxes_agg.append(boxes.float())\n",
    "                boxes_prev_agg.append(boxes_prev.float())\n",
    "                velocities_agg.append(velocities.float())\n",
    "                num_boxes_mask_agg.append(num_boxes_mask.bool())\n",
    "                gt_ids_agg.append(gt_ids)\n",
    "                gt_ids_prev_agg.append(gt_ids_prev)\n",
    "                \n",
    "            # convert aggregate lists to torch tensors\n",
    "            if self.window_length > 1:\n",
    "                frame_agg = [t.unsqueeze(0) for t in frame_agg]\n",
    "                motion_vectors_agg = [t.unsqueeze(0) for t in motion_vectors_agg]\n",
    "                boxes_agg = [t.unsqueeze(0) for t in boxes_agg]\n",
    "                boxes_prev_agg = [t.unsqueeze(0) for t in boxes_prev_agg]\n",
    "                velocities_agg = [t.unsqueeze(0) for t in velocities_agg]\n",
    "                num_boxes_mask_agg = [t.unsqueeze(0) for t in num_boxes_mask_agg]\n",
    "                gt_ids_agg = [t.unsqueeze(0) for t in gt_ids_agg]\n",
    "                gt_ids_prev_agg = [t.unsqueeze(0) for t in gt_ids_prev_agg]\n",
    "            \n",
    "            frame_ret = torch.cat(frame_agg, axis=0)\n",
    "            motion_vectors_ret = torch.cat(motion_vectors_agg, axis=0)\n",
    "            boxes_ret = torch.cat(boxes_agg, axis=0)\n",
    "            boxes_prev_ret = torch.cat(boxes_prev_agg, axis=0)\n",
    "            velocities_ret = torch.cat(velocities_agg, axis=0)\n",
    "            num_boxes_mask_ret = torch.cat(num_boxes_mask_agg, axis=0)\n",
    "            gt_ids_ret = torch.cat(gt_ids_agg, axis=0)\n",
    "            gt_ids_prev_ret = torch.cat(gt_ids_prev_agg, axis=0)\n",
    "            \n",
    "            # TODO: remove I frames\n",
    "            \n",
    "            return motion_vectors_ret, boxes_prev_ret, velocities_ret, num_boxes_mask_ret, boxes_ret, gt_ids_ret, gt_ids_prev_ret, frame_ret\n",
    "        \n",
    "\n",
    "datasets = {x: MotionVectorDataset(root_dir='../benchmark/MOT17', window_length=1, codec=\"mpeg4\", mode=x) for x in [\"train\", \"val\", \"test\"]}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=2, shuffle=False, num_workers=8) for x in [\"train\", \"val\", \"test\"]}\n",
    "\n",
    "for step, (motion_vectors, boxes_prev, velocities, num_boxes_mask, boxes, gt_ids, gt_ids_prev, frames) in enumerate(dataloaders[\"train\"]):\n",
    "    \n",
    "    print(step)\n",
    "    print(frames.shape)\n",
    "    print(motion_vectors.shape)\n",
    "    print(boxes_prev.shape)\n",
    "    print(boxes.shape)    \n",
    "    print(velocities.shape)\n",
    "    print(num_boxes_mask.shape)\n",
    "    print(gt_ids.shape)\n",
    "    print(gt_ids_prev.shape)\n",
    "    print(motion_vectors)\n",
    "    print(boxes_prev[num_boxes_mask])\n",
    "    print(boxes[num_boxes_mask])    \n",
    "    print(velocities[num_boxes_mask])\n",
    "    print(gt_ids)\n",
    "    print(gt_ids_prev)\n",
    "    plt.imshow(frames[0, 0, :, :, :])\n",
    "\n",
    "    plt.quiver(motion_vectors[0, 0, :, :], motion_vectors[0, 1, :, :], scale=1000)\n",
    "    plt.show()\n",
    "    \n",
    "    if step > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class MotionVectorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, mode, keyframe_interval=10, pad_num_boxes=52):\n",
    "        self.mode = mode\n",
    "        self.keyframe_interval = keyframe_interval\n",
    "        self.pad_num_boxes = pad_num_boxes\n",
    "        data_file = os.path.join(root_dir, \"preprocessed\", mode, \"data.pkl\")\n",
    "        self.data = pickle.load(open(data_file, \"rb\"))\n",
    "               \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.mode == \"train\" or self.mode == \"val\":\n",
    "            \n",
    "            motion_vectors = self.data[idx][\"motion_vectors\"]\n",
    "            \n",
    "            frame_idx = self.data[idx][\"frame_idx_no_skip\"]\n",
    "            gt_ids = self.data[idx][\"gt_ids\"]\n",
    "            gt_boxes = self.data[idx][\"gt_boxes\"]\n",
    "            gt_ids_prev = self.data[idx][\"gt_ids_prev\"]\n",
    "            gt_boxes_prev = self.data[idx][\"gt_boxes_prev\"]\n",
    "\n",
    "            # find boxes which occured in the last frame\n",
    "            _, idx_1, idx_0 = np.intersect1d(gt_ids, gt_ids_prev, assume_unique=True, return_indices=True)\n",
    "            boxes = torch.from_numpy(gt_boxes[idx_1])\n",
    "            boxes_prev = torch.from_numpy(gt_boxes_prev[idx_0])\n",
    "            velocities = velocities_from_boxes(boxes_prev, boxes)\n",
    "            \n",
    "            # insert frame index into boxes\n",
    "            num_boxes = (boxes.shape)[0]\n",
    "            boxes_prev_tmp = torch.zeros(num_boxes, 5)\n",
    "            boxes_prev_tmp[:, 1:5] = boxes_prev\n",
    "            boxes_prev_tmp[:, 0] = torch.full((num_boxes,), frame_idx)\n",
    "            boxes_prev = boxes_prev_tmp\n",
    "            \n",
    "            # pad boxes to the same global length (for MOT17 this is 52)\n",
    "            boxes_prev_padded = torch.zeros(self.pad_num_boxes, 5)\n",
    "            boxes_prev_padded[:num_boxes, :] = boxes_prev\n",
    "            boxes_prev = boxes_prev_padded\n",
    "            \n",
    "            # similarly pad velocites\n",
    "            velocities_padded = torch.zeros(self.pad_num_boxes, 4)\n",
    "            velocities_padded[:num_boxes, :] = velocities\n",
    "            velocities = velocities_padded\n",
    "            \n",
    "            # create a mask to revert the padding at a later stage\n",
    "            num_boxes_mask = torch.zeros(self.pad_num_boxes,)\n",
    "            num_boxes_mask[0:num_boxes] = torch.ones(num_boxes,)\n",
    "            \n",
    "            return motion_vectors.float(), boxes_prev.float(), velocities.float(), num_boxes_mask.bool()\n",
    "        \n",
    "        # TODO: handle test case"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "each sample must contain\n",
    "\n",
    "train/val:\n",
    "\n",
    "x: MV_t, B_{t-1}\n",
    "y: v_t\n",
    "\n",
    "boxes must be ordered consistently since last keyframe, only boxes with same gt_ids must be considered\n",
    "\n",
    "test:\n",
    "\n",
    "x: MV_t, Bd_{0} where t = 0 is the last keyframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagation Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropagationNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PropagationNetwork, self).__init__()\n",
    "        \n",
    "        self.POOLING_SIZE = 7  # the ROIs are split into m x m regions\n",
    "        self.FIXED_BLOCKS = 1\n",
    "        \n",
    "        self.base = torchvision.models.resnet18(pretrained=True)\n",
    "        \n",
    "        # change number of input channels from 3 to 2\n",
    "        #self.base.conv1.in_channels = 2\n",
    "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "         \n",
    "        # remove fully connected and avg pool layers\n",
    "        self.base = nn.Sequential(*list(self.base.children())[:-2])\n",
    "        \n",
    "        # change stride to 1 in conv5 block\n",
    "        #self.base[5][0].conv1 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), dilation=2, padding=(1, 1), bias=False)\n",
    "        #self.base[5][0].conv2 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), dilation=2, padding=(1, 1), bias=False)\n",
    "        \n",
    "        #self.conv1 = nn.Conv2d(512, 4, kernel_size=(1, 1), stride=(1, 1))\n",
    "        self.conv1 = nn.Conv2d(512, 4*self.POOLING_SIZE*self.POOLING_SIZE, kernel_size=(1, 1), stride=(1, 1), padding=0, bias=False)\n",
    "        \n",
    "        \n",
    "        #def set_bn_fix(m):\n",
    "        #    classname = m.__class__.__name__\n",
    "        #    print(classname)\n",
    "        #    if classname.find('BatchNorm2d') != -1:\n",
    "        #        for p in m.parameters(): p.requires_grad = False\n",
    "        #\n",
    "        #self.base.apply(set_bn_fix)\n",
    "        \n",
    "        assert (0 <= self.FIXED_BLOCKS <= 4) # set this value to 0, so we can train all blocks\n",
    "        if self.FIXED_BLOCKS >= 4: # fix all blocks\n",
    "            for p in self.base[10].parameters(): p.requires_grad = False\n",
    "        if self.FIXED_BLOCKS >= 3: # fix first 3 blocks\n",
    "            for p in self.base[8].parameters(): p.requires_grad = False\n",
    "        if self.FIXED_BLOCKS >= 2: # fix first 2 blocks\n",
    "            for p in self.base[6].parameters(): p.requires_grad = False\n",
    "        if self.FIXED_BLOCKS >= 1: # fix first 1 block\n",
    "            for p in self.base[4].parameters(): p.requires_grad = False\n",
    "        \n",
    "        #print([p.requires_grad for p in self.base.parameters()])\n",
    "        \n",
    "        \n",
    "        #print(list(self.base.children())[5][0].conv1)\n",
    "        #print(list(zip(list(self.children()), [p.requires_grad for p in self.base.parameters()])))\n",
    "        \n",
    "    def forward(self, motion_vectors, boxes_prev, num_boxes_mask):\n",
    "        #print(boxes_prev)\n",
    "        #print(\"boxes_prev:\", boxes_prev.shape)\n",
    "        #print(\"motion_vectors:\", motion_vectors.shape)\n",
    "        x = self.base(motion_vectors)\n",
    "        #print(\"after ResNet18:\", x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        #print(\"after conv1\", x.shape)\n",
    "        \n",
    "        boxes_prev = self._change_box_format(boxes_prev)\n",
    "        boxes_prev = boxes_prev[num_boxes_mask]\n",
    "        boxes_prev = boxes_prev.view(-1, 5)\n",
    "        # offset frame_idx so that it corresponds to batch index\n",
    "        boxes_prev[..., :, 0] = boxes_prev[..., :, 0] - boxes_prev[..., 0, 0]\n",
    "        \n",
    "        # compute ratio of input size to size of base output\n",
    "        spatial_scale = x.shape[-1] / (motion_vectors.shape)[-1]\n",
    "        print(spatial_scale)\n",
    "        x = torchvision.ops.ps_roi_pool(x, boxes_prev, output_size=(self.POOLING_SIZE, self.POOLING_SIZE), spatial_scale=spatial_scale)\n",
    "        #print(\"after roi_pool\", x.shape)\n",
    "        velocities_pred = x.mean(-1).mean(-1)\n",
    "        #print(\"after averaging\", velocities_pred.shape)\n",
    "        \n",
    "        return velocities_pred\n",
    "    \n",
    "    \n",
    "    def _change_box_format(self, boxes):\n",
    "        \"\"\"Change format of boxes from [x, y, w, h] to [x1, y1, x2, y2].\"\"\"\n",
    "        boxes[..., 0] = boxes[..., 0]\n",
    "        boxes[..., 1] = boxes[..., 1]\n",
    "        boxes[..., 2] = boxes[..., 2]\n",
    "        boxes[..., 3] = boxes[..., 1] + boxes[..., 3]\n",
    "        boxes[..., 4] = boxes[..., 2] + boxes[..., 4]\n",
    "        return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, num_epochs=2):\n",
    "    tstart = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    pickle.dump(best_model_wts, open(\"models/best_model.pkl\", \"wb\"))\n",
    "    best_loss = 99999.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs-1))\n",
    "        \n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "\n",
    "            for step, (motion_vectors, boxes_prev, velocities, num_boxes_mask) in enumerate(dataloaders[phase]):\n",
    "                motion_vectors = motion_vectors.to(device)\n",
    "                boxes_prev = boxes_prev.to(device)\n",
    "                velocities = velocities.to(device)\n",
    "                num_boxes_mask = num_boxes_mask.to(device)\n",
    "                \n",
    "                velocities = velocities[num_boxes_mask]\n",
    "                velocities = velocities.view(-1, 4)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    velocities_pred = model(motion_vectors, boxes_prev, num_boxes_mask)\n",
    "                    \n",
    "                    loss = criterion(velocities_pred, velocities)\n",
    "                    \n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                       \n",
    "                running_loss += loss.item() * motion_vectors.size(0)\n",
    "                \n",
    "            epoch_loss = running_loss / len(datasets[phase])\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            \n",
    "            if phase == \"val\":\n",
    "                model_wts = copy.deepcopy(model.state_dict())\n",
    "                pickle.dump(best_model_wts, open(\"models/model_{:04d}.pkl\".format(epoch), \"wb\"))\n",
    "            \n",
    "            if phase == \"val\" and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                pickle.dump(best_model_wts, open(\"models/best_model.pkl\", \"wb\"))\n",
    "                \n",
    "    time_elapsed = time.time() - tstart\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Lowest validation loss: {:4f}'.format(best_loss))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 1/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 2/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 3/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 4/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 5/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 6/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 7/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 8/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 9/59\n"
     ]
    }
   ],
   "source": [
    "datasets = {x: MotionVectorDataset(root_dir='../benchmark/MOT17', mode=x) for x in [\"train\", \"val\", \"test\"]}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=2, shuffle=False, num_workers=4) for x in [\"train\", \"val\", \"test\"]}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PropagationNetwork()\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.SmoothL1Loss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)\n",
    "best_model = train(model, criterion, optimizer, scheduler, num_epochs=60)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# test cases for utility functions\n",
    "gt_boxes_prev = np.array([[1338.,  418.,  167.,  379.],\n",
    "          [ 586.,  447.,   85.,  263.],\n",
    "          [1416.,  431.,  184.,  336.],\n",
    "          [1056.,  484.,   36.,  110.],\n",
    "          [1091.,  484.,   31.,  115.],\n",
    "          [1255.,  447.,   33.,  100.],\n",
    "          [1016.,  430.,   40.,  116.],\n",
    "          [1101.,  441.,   38.,  108.],\n",
    "          [ 935.,  436.,   42.,  114.],\n",
    "          [ 442.,  446.,  105.,  283.],\n",
    "          [ 636.,  458.,   61.,  187.],\n",
    "          [1364.,  434.,   51.,  124.],\n",
    "          [1478.,  434.,   63.,  124.],\n",
    "          [ 473.,  460.,   89.,  249.],\n",
    "          [ 548.,  465.,   35.,   93.],\n",
    "          [ 418.,  459.,   40.,   84.],\n",
    "          [ 582.,  456.,   35.,  133.],\n",
    "          [ 972.,  456.,   32.,   77.],\n",
    "          [ 578.,  432.,   20.,   43.],\n",
    "          [ 596.,  429.,   18.,   42.],\n",
    "          [ 663.,  451.,   34.,   86.]])\n",
    "\n",
    "gt_boxes = np.array([[1342.,  417.,  168.,  380.],\n",
    "          [ 586.,  446.,   85.,  264.],\n",
    "          [1422.,  431.,  183.,  337.],\n",
    "          [1055.,  483.,   36.,  110.],\n",
    "          [1090.,  484.,   32.,  114.],\n",
    "          [1255.,  447.,   33.,  100.],\n",
    "          [1015.,  430.,   40.,  116.],\n",
    "          [1100.,  440.,   38.,  108.],\n",
    "          [ 934.,  435.,   42.,  114.],\n",
    "          [ 442.,  446.,  107.,  282.],\n",
    "          [ 636.,  458.,   61.,  187.],\n",
    "          [1365.,  434.,   52.,  124.],\n",
    "          [1480.,  433.,   62.,  125.],\n",
    "          [ 473.,  460.,   89.,  249.],\n",
    "          [ 547.,  464.,   35.,   93.],\n",
    "          [ 418.,  459.,   40.,   84.],\n",
    "          [ 582.,  455.,   34.,  134.],\n",
    "          [ 972.,  456.,   32.,   77.],\n",
    "          [ 578.,  431.,   20.,   43.],\n",
    "          [ 595.,  428.,   18.,   42.],\n",
    "          [1035.,  452.,   25.,   67.],\n",
    "          [ 664.,  451.,   34.,   85.]])\n",
    "\n",
    "gt_ids = np.array([ 2.,  3.,  8.,  9., 10., 14., 15., 17., 18., 19., 20., 21., 22., 23.,\n",
    "          26., 31., 36., 39., 68., 69., 70., 72.])\n",
    "\n",
    "gt_ids_prev = np.array([ 2.,  3.,  8.,  9., 10., 14., 15., 17., 18., 19., 20., 21., 22., 23.,\n",
    "          26., 31., 36., 39., 68., 69., 72.])\n",
    "\n",
    "_, idx_1, idx_0 = np.intersect1d(gt_ids, gt_ids_prev, assume_unique=True, return_indices=True)\n",
    "print(idx_1, idx_0)\n",
    "boxes = torch.from_numpy(gt_boxes[idx_1]).unsqueeze(0)\n",
    "boxes = torch.cat([boxes, boxes], axis=0)\n",
    "boxes_prev = torch.from_numpy(gt_boxes_prev[idx_0]).unsqueeze(0)\n",
    "boxes_prev = torch.cat([boxes_prev, boxes_prev], axis=0)\n",
    "print(boxes.shape)\n",
    "print(boxes_prev.shape)\n",
    "\n",
    "velocities = velocities_from_boxes(boxes_prev, boxes)\n",
    "print(velocities)\n",
    "print(velocities.shape)\n",
    "\n",
    "\n",
    "box = box_from_velocities(boxes_prev, velocities)\n",
    "print(box)\n",
    "print(box.shape)\n",
    "print(boxes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mvs_x = (item[\"x\"].numpy())[:, :, 0]\n",
    "mvs_y = (item[\"x\"].numpy())[:, :, 1]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(25,15))\n",
    "ax.quiver(xi, yi, mvs_x, mvs_y, scale=1000, color='r')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input:\n",
    "    - motion vectors at frame t-w, t-w+1, t-w+2,... t-1, t (window length w)\n",
    "    - bounding box coordinates (x, y, w, h) at frame t-w, t-w+1, t-w+2,... t-1\n",
    "    \n",
    "Output:\n",
    "    - bounding box coordinates for frame t\n",
    "    \n",
    "Difficulties:\n",
    "    - keyframes do not have any MVs\n",
    "    - number of bounding boxes in frame can vary over time\n",
    "    - number of vectors inside bounding box can vary over time\n",
    "    - size of bounding box changes over time\n",
    "    \n",
    "Ideas:\n",
    "    1) Encode each vector as magnitude and direction\n",
    "    2) Extrapolate vector values on evenly spaced 16 x 16 grid\n",
    "    3) Use CNN-LSTM to encode the sequence of vectors and use attention to guide to bounding box region only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
