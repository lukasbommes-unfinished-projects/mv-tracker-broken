{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def velocities_from_boxes(boxes_prev, boxes):\n",
    "    \"\"\"Computes bounding box velocities.\n",
    "    \n",
    "    Args:\n",
    "        boxes_prev (`torch.Tensor`): Bounding boxes in previous frame. Shape [B, N, 4]\n",
    "            where B is the batch size and N the number of bounding boxes.\n",
    "            \n",
    "        boxes (`torch.Tensor`): Bounding boxes in current frame. Shape [B, N, 4]\n",
    "            where B is the batch size and N the number of bounding boxes.\n",
    "            \n",
    "    Returns:\n",
    "        (`torch.Tensor`) velocities of box coordinates in current frame. Shape [B, N, 4].\n",
    "        \n",
    "    Ensure that ordering of boxes in both tensors is consistent and that the number of boxes\n",
    "    is the same.\n",
    "    \"\"\"\n",
    "    x = boxes[:, 0]\n",
    "    y = boxes[:, 1]\n",
    "    w = boxes[:, 2]\n",
    "    h = boxes[:, 3]\n",
    "    x_p = boxes_prev[:, 0]\n",
    "    y_p = boxes_prev[:, 1]\n",
    "    w_p = boxes_prev[:, 2]\n",
    "    h_p = boxes_prev[:, 3] \n",
    "    v_x = (1 / w_p * (x - x_p)).unsqueeze(-1)\n",
    "    v_y = (1 / h_p * (y - y_p)).unsqueeze(-1)\n",
    "    v_w = (torch.log(w / w_p)).unsqueeze(-1)\n",
    "    v_h = (torch.log(h / h_p)).unsqueeze(-1) \n",
    "    return torch.cat([v_x, v_y, v_w, v_h], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_from_velocities(boxes_prev, velocities):\n",
    "    \"\"\"Computes bounding boxes from previous boxes and velocities.\n",
    "    \n",
    "    Args:\n",
    "        boxes_prev (`torch.Tensor`): Bounding boxes in previous frame. Shape [B, N, 4] \n",
    "            where B is the batch size and N the number of bounding boxes.\n",
    "            \n",
    "        velocities (`torch.Tensor`): Box velocities in current frame. Shape [B, N, 4] \n",
    "        where B is the batch size and N the number of bounding boxes.\n",
    "            \n",
    "    Returns:\n",
    "        (`torch.Tensor`) Bounding boxes in current frame. Shape [B, N, 4].\n",
    "        \n",
    "    Ensure that ordering of boxes and velocities in both tensors is consistent that is\n",
    "    box in row i should correspond to velocities in row i.\n",
    "    \"\"\"\n",
    "    x_p = boxes_prev[:, 0]\n",
    "    y_p = boxes_prev[:, 1]\n",
    "    w_p = boxes_prev[:, 2]\n",
    "    h_p = boxes_prev[:, 3]\n",
    "    v_x = velocities[:, 0]\n",
    "    v_y = velocities[:, 1]\n",
    "    v_w = velocities[:, 2]\n",
    "    v_h = velocities[:, 3]\n",
    "    x = (w_p * v_x + x_p).unsqueeze(-1)\n",
    "    y = (h_p * v_y + y_p).unsqueeze(-1)\n",
    "    w = (w_p * torch.exp(v_w)).unsqueeze(-1)\n",
    "    h = (h_p * torch.exp(v_h)).unsqueeze(-1)\n",
    "    return torch.cat([x, y, w, h], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([2, 1080, 1920, 3])\n",
      "torch.Size([2, 2, 68, 120])\n",
      "torch.Size([2, 52, 5])\n",
      "torch.Size([2, 52, 4])\n",
      "torch.Size([2, 52, 4])\n",
      "torch.Size([2, 52])\n",
      "torch.Size([2, 52])\n",
      "torch.Size([2, 52])\n",
      "tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.5000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.5000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.5000,  0.5000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000, -0.5000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.5000,  0.5000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]])\n",
      "tensor([[1.0000e+00, 1.3380e+03, 4.1800e+02, 1.6700e+02, 3.7900e+02],\n",
      "        [1.0000e+00, 5.8600e+02, 4.4700e+02, 8.5000e+01, 2.6300e+02],\n",
      "        [1.0000e+00, 1.4160e+03, 4.3100e+02, 1.8400e+02, 3.3600e+02],\n",
      "        [1.0000e+00, 1.0560e+03, 4.8400e+02, 3.6000e+01, 1.1000e+02],\n",
      "        [1.0000e+00, 1.0910e+03, 4.8400e+02, 3.1000e+01, 1.1500e+02],\n",
      "        [1.0000e+00, 1.2550e+03, 4.4700e+02, 3.3000e+01, 1.0000e+02],\n",
      "        [1.0000e+00, 1.0160e+03, 4.3000e+02, 4.0000e+01, 1.1600e+02],\n",
      "        [1.0000e+00, 1.1010e+03, 4.4100e+02, 3.8000e+01, 1.0800e+02],\n",
      "        [1.0000e+00, 9.3500e+02, 4.3600e+02, 4.2000e+01, 1.1400e+02],\n",
      "        [1.0000e+00, 4.4200e+02, 4.4600e+02, 1.0500e+02, 2.8300e+02],\n",
      "        [1.0000e+00, 6.3600e+02, 4.5800e+02, 6.1000e+01, 1.8700e+02],\n",
      "        [1.0000e+00, 1.3640e+03, 4.3400e+02, 5.1000e+01, 1.2400e+02],\n",
      "        [1.0000e+00, 1.4780e+03, 4.3400e+02, 6.3000e+01, 1.2400e+02],\n",
      "        [1.0000e+00, 4.7300e+02, 4.6000e+02, 8.9000e+01, 2.4900e+02],\n",
      "        [1.0000e+00, 5.4800e+02, 4.6500e+02, 3.5000e+01, 9.3000e+01],\n",
      "        [1.0000e+00, 4.1800e+02, 4.5900e+02, 4.0000e+01, 8.4000e+01],\n",
      "        [1.0000e+00, 5.8200e+02, 4.5600e+02, 3.5000e+01, 1.3300e+02],\n",
      "        [1.0000e+00, 9.7200e+02, 4.5600e+02, 3.2000e+01, 7.7000e+01],\n",
      "        [1.0000e+00, 5.7800e+02, 4.3200e+02, 2.0000e+01, 4.3000e+01],\n",
      "        [1.0000e+00, 5.9600e+02, 4.2900e+02, 1.8000e+01, 4.2000e+01],\n",
      "        [1.0000e+00, 1.0360e+03, 4.5300e+02, 2.5000e+01, 6.7000e+01],\n",
      "        [1.0000e+00, 6.6300e+02, 4.5100e+02, 3.4000e+01, 8.6000e+01],\n",
      "        [2.0000e+00, 1.3420e+03, 4.1700e+02, 1.6800e+02, 3.8000e+02],\n",
      "        [2.0000e+00, 5.8600e+02, 4.4600e+02, 8.5000e+01, 2.6400e+02],\n",
      "        [2.0000e+00, 1.4220e+03, 4.3100e+02, 1.8300e+02, 3.3700e+02],\n",
      "        [2.0000e+00, 1.0550e+03, 4.8300e+02, 3.6000e+01, 1.1000e+02],\n",
      "        [2.0000e+00, 1.0900e+03, 4.8400e+02, 3.2000e+01, 1.1400e+02],\n",
      "        [2.0000e+00, 1.2550e+03, 4.4700e+02, 3.3000e+01, 1.0000e+02],\n",
      "        [2.0000e+00, 1.0150e+03, 4.3000e+02, 4.0000e+01, 1.1600e+02],\n",
      "        [2.0000e+00, 1.1000e+03, 4.4000e+02, 3.8000e+01, 1.0800e+02],\n",
      "        [2.0000e+00, 9.3400e+02, 4.3500e+02, 4.2000e+01, 1.1400e+02],\n",
      "        [2.0000e+00, 4.4200e+02, 4.4600e+02, 1.0700e+02, 2.8200e+02],\n",
      "        [2.0000e+00, 6.3600e+02, 4.5800e+02, 6.1000e+01, 1.8700e+02],\n",
      "        [2.0000e+00, 1.3650e+03, 4.3400e+02, 5.2000e+01, 1.2400e+02],\n",
      "        [2.0000e+00, 1.4800e+03, 4.3300e+02, 6.2000e+01, 1.2500e+02],\n",
      "        [2.0000e+00, 4.7300e+02, 4.6000e+02, 8.9000e+01, 2.4900e+02],\n",
      "        [2.0000e+00, 5.4700e+02, 4.6400e+02, 3.5000e+01, 9.3000e+01],\n",
      "        [2.0000e+00, 4.1800e+02, 4.5900e+02, 4.0000e+01, 8.4000e+01],\n",
      "        [2.0000e+00, 5.8200e+02, 4.5500e+02, 3.4000e+01, 1.3400e+02],\n",
      "        [2.0000e+00, 9.7200e+02, 4.5600e+02, 3.2000e+01, 7.7000e+01],\n",
      "        [2.0000e+00, 5.7800e+02, 4.3100e+02, 2.0000e+01, 4.3000e+01],\n",
      "        [2.0000e+00, 5.9500e+02, 4.2800e+02, 1.8000e+01, 4.2000e+01],\n",
      "        [2.0000e+00, 1.0350e+03, 4.5200e+02, 2.5000e+01, 6.7000e+01],\n",
      "        [2.0000e+00, 6.6400e+02, 4.5100e+02, 3.4000e+01, 8.5000e+01]])\n",
      "tensor([[1342.,  417.,  168.,  380.],\n",
      "        [ 586.,  446.,   85.,  264.],\n",
      "        [1422.,  431.,  183.,  337.],\n",
      "        [1055.,  483.,   36.,  110.],\n",
      "        [1090.,  484.,   32.,  114.],\n",
      "        [1255.,  447.,   33.,  100.],\n",
      "        [1015.,  430.,   40.,  116.],\n",
      "        [1100.,  440.,   38.,  108.],\n",
      "        [ 934.,  435.,   42.,  114.],\n",
      "        [ 442.,  446.,  107.,  282.],\n",
      "        [ 636.,  458.,   61.,  187.],\n",
      "        [1365.,  434.,   52.,  124.],\n",
      "        [1480.,  433.,   62.,  125.],\n",
      "        [ 473.,  460.,   89.,  249.],\n",
      "        [ 547.,  464.,   35.,   93.],\n",
      "        [ 418.,  459.,   40.,   84.],\n",
      "        [ 582.,  455.,   34.,  134.],\n",
      "        [ 972.,  456.,   32.,   77.],\n",
      "        [ 578.,  431.,   20.,   43.],\n",
      "        [ 595.,  428.,   18.,   42.],\n",
      "        [1035.,  452.,   25.,   67.],\n",
      "        [ 664.,  451.,   34.,   85.],\n",
      "        [1346.,  417.,  170.,  380.],\n",
      "        [ 586.,  446.,   85.,  264.],\n",
      "        [1428.,  431.,  182.,  338.],\n",
      "        [1055.,  483.,   36.,  110.],\n",
      "        [1090.,  484.,   32.,  114.],\n",
      "        [1255.,  447.,   33.,  100.],\n",
      "        [1015.,  430.,   40.,  116.],\n",
      "        [1100.,  440.,   38.,  108.],\n",
      "        [ 934.,  435.,   42.,  114.],\n",
      "        [ 442.,  446.,  109.,  282.],\n",
      "        [ 636.,  458.,   62.,  187.],\n",
      "        [1366.,  434.,   54.,  124.],\n",
      "        [1483.,  433.,   60.,  125.],\n",
      "        [ 474.,  460.,   89.,  249.],\n",
      "        [ 547.,  464.,   35.,   93.],\n",
      "        [ 418.,  459.,   40.,   84.],\n",
      "        [ 582.,  455.,   34.,  134.],\n",
      "        [ 973.,  456.,   32.,   77.],\n",
      "        [ 578.,  431.,   20.,   43.],\n",
      "        [ 595.,  428.,   18.,   42.],\n",
      "        [1035.,  452.,   25.,   67.],\n",
      "        [ 665.,  451.,   34.,   85.]])\n",
      "tensor([[ 0.0240, -0.0026,  0.0060,  0.0026],\n",
      "        [ 0.0000, -0.0038,  0.0000,  0.0038],\n",
      "        [ 0.0326,  0.0000, -0.0054,  0.0030],\n",
      "        [-0.0278, -0.0091,  0.0000,  0.0000],\n",
      "        [-0.0323,  0.0000,  0.0317, -0.0087],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0250,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0263, -0.0093,  0.0000,  0.0000],\n",
      "        [-0.0238, -0.0088,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0189, -0.0035],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0196,  0.0000,  0.0194,  0.0000],\n",
      "        [ 0.0317, -0.0081, -0.0160,  0.0080],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0286, -0.0108,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0075, -0.0290,  0.0075],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0233,  0.0000,  0.0000],\n",
      "        [-0.0556, -0.0238,  0.0000,  0.0000],\n",
      "        [-0.0400, -0.0149,  0.0000,  0.0000],\n",
      "        [ 0.0294,  0.0000,  0.0000, -0.0117],\n",
      "        [ 0.0238,  0.0000,  0.0118,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0328,  0.0000, -0.0055,  0.0030],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0185,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0163,  0.0000],\n",
      "        [ 0.0192,  0.0000,  0.0377,  0.0000],\n",
      "        [ 0.0484,  0.0000, -0.0328,  0.0000],\n",
      "        [ 0.0112,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0312,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0294,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[ 2.,  3.,  8.,  9., 10., 14., 15., 17., 18., 19., 20., 21., 22., 23.,\n",
      "         26., 31., 36., 39., 68., 69., 70., 72.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 2.,  3.,  8.,  9., 10., 14., 15., 17., 18., 19., 20., 21., 22., 23.,\n",
      "         26., 31., 36., 39., 68., 69., 70., 72.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "tensor([[ 2.,  3.,  8.,  9., 10., 14., 15., 17., 18., 19., 20., 21., 22., 23.,\n",
      "         26., 31., 36., 39., 68., 69., 70., 72.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 2.,  3.,  8.,  9., 10., 14., 15., 17., 18., 19., 20., 21., 22., 23.,\n",
      "         26., 31., 36., 39., 68., 69., 70., 72.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-30251d1823dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_ids_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquiver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmotion_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmotion_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 4"
     ]
    }
   ],
   "source": [
    "class MotionVectorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, codec, mode, keyframe_interval=10, pad_num_boxes=52, window_length=3):\n",
    "        self.root_dir = root_dir\n",
    "        self.mode = mode\n",
    "        self.keyframe_interval = keyframe_interval\n",
    "        self.pad_num_boxes = pad_num_boxes\n",
    "        self.window_length = window_length\n",
    "        assert self.window_length > 0, \"window length must be 1 or greater\"\n",
    "        data_file = os.path.join(root_dir, \"preprocessed\", codec, mode, \"data.pkl\")\n",
    "        self.data = pickle.load(open(data_file, \"rb\"))\n",
    "        \n",
    "        # remove entries so that an integer number of sequences of window length can be generated\n",
    "        if self.window_length > 1:\n",
    "            lengths_file = os.path.join(root_dir, \"preprocessed\", codec, mode, \"lengths.pkl\")\n",
    "            lengths = pickle.load(open(lengths_file, \"rb\"))\n",
    "            lenghts_cumsum = np.cumsum(lengths)\n",
    "            for lenght, lenght_cumsum in zip(reversed(lengths), reversed(lenghts_cumsum)):\n",
    "                remainder = lenght % self.window_length  # e.g. 2\n",
    "                for r in range(remainder):\n",
    "                    self.data.pop(lenght_cumsum-1-r)\n",
    "                    \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data) - 1  # -1 is needed because of idx+1 in __getitem__\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.mode == \"train\" or self.mode == \"val\":\n",
    "            \n",
    "            frame_agg = []\n",
    "            motion_vectors_agg = []\n",
    "            boxes_agg = []\n",
    "            boxes_prev_agg = []\n",
    "            gt_ids_agg = []\n",
    "            gt_ids_prev_agg = []\n",
    "            velocities_agg = []\n",
    "            num_boxes_mask_agg = []\n",
    "            \n",
    "            # for every sample we expect 3 (window_length) entries\n",
    "            for ws in range(self.window_length):  # ws = 0, 1, 2\n",
    "                \n",
    "                frame_idx = self.data[idx + ws + 1][\"frame_idx\"]\n",
    "            \n",
    "                motion_vectors = self.data[idx + ws + 1][\"motion_vectors\"]\n",
    "                \n",
    "                # get current frame\n",
    "                sequence = self.data[idx + ws + 1][\"sequence\"]\n",
    "                frame_file = os.path.join(self.root_dir, \"train\", \"{}-FRCNN/img1/{:06d}.jpg\".format(sequence, frame_idx+1))\n",
    "                frame = torch.from_numpy(cv2.imread(frame_file, cv2.IMREAD_COLOR))\n",
    "\n",
    "                gt_ids = self.data[idx + ws + 1][\"gt_ids\"]\n",
    "                gt_boxes = self.data[idx + ws + 1][\"gt_boxes\"]\n",
    "                gt_ids_prev = self.data[idx + ws][\"gt_ids\"]\n",
    "                gt_boxes_prev = self.data[idx + ws][\"gt_boxes\"]\n",
    "\n",
    "                # find boxes which occured in the last frame\n",
    "                _, idx_1, idx_0 = np.intersect1d(gt_ids, gt_ids_prev, assume_unique=True, return_indices=True)\n",
    "                boxes = torch.from_numpy(gt_boxes[idx_1])\n",
    "                boxes_prev = torch.from_numpy(gt_boxes_prev[idx_0])\n",
    "                velocities = velocities_from_boxes(boxes_prev, boxes)\n",
    "                gt_ids = torch.from_numpy(gt_ids[idx_1])\n",
    "                gt_ids_prev = torch.from_numpy(gt_ids_prev[idx_0])\n",
    "\n",
    "                # insert frame index into boxes\n",
    "                num_boxes = (boxes.shape)[0]\n",
    "                boxes_prev_tmp = torch.zeros(num_boxes, 5)\n",
    "                boxes_prev_tmp[:, 1:5] = boxes_prev\n",
    "                boxes_prev_tmp[:, 0] = torch.full((num_boxes,), frame_idx)\n",
    "                boxes_prev = boxes_prev_tmp\n",
    "\n",
    "                # pad boxes_prev to the same global length (for MOT17 this is 52)\n",
    "                boxes_prev_padded = torch.zeros(self.pad_num_boxes, 5)\n",
    "                boxes_prev_padded[:num_boxes, :] = boxes_prev\n",
    "                boxes_prev = boxes_prev_padded\n",
    "                \n",
    "                # similarly pad boxes\n",
    "                boxes_padded = torch.zeros(self.pad_num_boxes, 4)\n",
    "                boxes_padded[:num_boxes, :] = boxes\n",
    "                boxes = boxes_padded\n",
    "\n",
    "                # similarly pad velocites\n",
    "                velocities_padded = torch.zeros(self.pad_num_boxes, 4)\n",
    "                velocities_padded[:num_boxes, :] = velocities\n",
    "                velocities = velocities_padded\n",
    "                \n",
    "                # similarly pad gt_ids\n",
    "                gt_ids_padded = torch.zeros(self.pad_num_boxes,)\n",
    "                gt_ids_padded[:num_boxes] = gt_ids\n",
    "                gt_ids = gt_ids_padded\n",
    "                gt_ids_prev_padded = torch.zeros(self.pad_num_boxes,)\n",
    "                gt_ids_prev_padded[:num_boxes] = gt_ids_prev\n",
    "                gt_ids_prev = gt_ids_prev_padded\n",
    "\n",
    "                # create a mask to revert the padding at a later stage\n",
    "                num_boxes_mask = torch.zeros(self.pad_num_boxes,)\n",
    "                num_boxes_mask[0:num_boxes] = torch.ones(num_boxes,)\n",
    "                \n",
    "                frame_agg.append(frame)\n",
    "                motion_vectors_agg.append(motion_vectors.float())\n",
    "                boxes_agg.append(boxes.float())\n",
    "                boxes_prev_agg.append(boxes_prev.float())\n",
    "                velocities_agg.append(velocities.float())\n",
    "                num_boxes_mask_agg.append(num_boxes_mask.bool())\n",
    "                gt_ids_agg.append(gt_ids)\n",
    "                gt_ids_prev_agg.append(gt_ids_prev)\n",
    "                \n",
    "            # convert aggregate lists to torch tensors\n",
    "            if self.window_length > 1:\n",
    "                frame_agg = [t.unsqueeze(0) for t in frame_agg]\n",
    "                motion_vectors_agg = [t.unsqueeze(0) for t in motion_vectors_agg]\n",
    "                boxes_agg = [t.unsqueeze(0) for t in boxes_agg]\n",
    "                boxes_prev_agg = [t.unsqueeze(0) for t in boxes_prev_agg]\n",
    "                velocities_agg = [t.unsqueeze(0) for t in velocities_agg]\n",
    "                num_boxes_mask_agg = [t.unsqueeze(0) for t in num_boxes_mask_agg]\n",
    "                gt_ids_agg = [t.unsqueeze(0) for t in gt_ids_agg]\n",
    "                gt_ids_prev_agg = [t.unsqueeze(0) for t in gt_ids_prev_agg]\n",
    "            \n",
    "            frame_ret = torch.cat(frame_agg, axis=0)\n",
    "            motion_vectors_ret = torch.cat(motion_vectors_agg, axis=0)\n",
    "            boxes_ret = torch.cat(boxes_agg, axis=0)\n",
    "            boxes_prev_ret = torch.cat(boxes_prev_agg, axis=0)\n",
    "            velocities_ret = torch.cat(velocities_agg, axis=0)\n",
    "            num_boxes_mask_ret = torch.cat(num_boxes_mask_agg, axis=0)\n",
    "            gt_ids_ret = torch.cat(gt_ids_agg, axis=0)\n",
    "            gt_ids_prev_ret = torch.cat(gt_ids_prev_agg, axis=0)\n",
    "            \n",
    "            # TODO: remove I frames\n",
    "            \n",
    "            return motion_vectors_ret, boxes_prev_ret, velocities_ret, num_boxes_mask_ret, boxes_ret, gt_ids_ret, gt_ids_prev_ret, frame_ret\n",
    "        \n",
    "\n",
    "datasets = {x: MotionVectorDataset(root_dir='../benchmark/MOT17', window_length=1, codec=\"mpeg4\", mode=x) for x in [\"train\", \"val\", \"test\"]}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=2, shuffle=False, num_workers=8) for x in [\"train\", \"val\", \"test\"]}\n",
    "\n",
    "for step, (motion_vectors, boxes_prev, velocities, num_boxes_mask, boxes, gt_ids, gt_ids_prev, frames) in enumerate(dataloaders[\"train\"]):\n",
    "    \n",
    "    print(step)\n",
    "    print(frames.shape)\n",
    "    print(motion_vectors.shape)\n",
    "    print(boxes_prev.shape)\n",
    "    print(boxes.shape)    \n",
    "    print(velocities.shape)\n",
    "    print(num_boxes_mask.shape)\n",
    "    print(gt_ids.shape)\n",
    "    print(gt_ids_prev.shape)\n",
    "    print(motion_vectors)\n",
    "    print(boxes_prev[num_boxes_mask])\n",
    "    print(boxes[num_boxes_mask])    \n",
    "    print(velocities[num_boxes_mask])\n",
    "    print(gt_ids)\n",
    "    print(gt_ids_prev)\n",
    "    plt.imshow(frames[0, 0, :, :, :])\n",
    "\n",
    "    plt.quiver(motion_vectors[0, 0, :, :], motion_vectors[0, 1, :, :], scale=1000)\n",
    "    plt.show()\n",
    "    \n",
    "    if step > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 68, 120])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motion_vectors_to_image(motion_vectors, frame_shape=(1920, 1080)):\n",
    "    mvs = motion_vectors[0, ...].numpy()\n",
    "    image = np.zeros((mvs.shape[1], mvs.shape[2], 3))\n",
    "    # # scale the components to range [0, 255]\n",
    "    # mvs_min = np.min(mvs, axis=(1, 2))\n",
    "    # mvs_max = np.max(mvs, axis=(1, 2))\n",
    "    # print(mvs_min, mvs_max)\n",
    "    # if (mvs_max[0] - mvs_min[0]) != 0 and (mvs_max[1] - mvs_min[1]) != 0:\n",
    "    #     mvs_x = (mvs[0, :, :] - mvs_min[0]) / (mvs_max[0] - mvs_min[0]) * 255\n",
    "    #     mvs_y = (mvs[1, :, :] - mvs_min[1]) / (mvs_max[1] - mvs_min[1]) * 255\n",
    "#    image[:, :, 2] = mvs[0, :, :]#mvs_x\n",
    "#    image[:, :, 1] = mvs[1, :, :]#mvs_y\n",
    "    \n",
    "    pixel_groups = np.aranage()\n",
    "    \n",
    "    image[:, :, 2]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvs_image = motion_vectors_to_image(motion_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1c9b0543c8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mvs_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1d0f21a668>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = np.zeros((108, 192, 3))\n",
    "\n",
    "def color_macroblock(image, x=0, y=0, value=127):\n",
    "    tlx = np.arange(16*x, 16*(x+1))\n",
    "    tly = np.arange(16*y, 16*(y+1))\n",
    "    xs, ys = np.meshgrid(tlx, tly)\n",
    "    #image[ys, xs, :] = 127\n",
    "    return image, xs, ys\n",
    "\n",
    "image, xs, ys = color_macroblock(image, x=11, y=6)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 96,  96,  96,  96,  96,  96,  96,  96,  96,  96,  96,  96,  96,\n",
       "         96,  96,  96],\n",
       "       [ 97,  97,  97,  97,  97,  97,  97,  97,  97,  97,  97,  97,  97,\n",
       "         97,  97,  97],\n",
       "       [ 98,  98,  98,  98,  98,  98,  98,  98,  98,  98,  98,  98,  98,\n",
       "         98,  98,  98],\n",
       "       [ 99,  99,  99,  99,  99,  99,  99,  99,  99,  99,  99,  99,  99,\n",
       "         99,  99,  99],\n",
       "       [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "        100, 100, 100],\n",
       "       [101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "        101, 101, 101],\n",
       "       [102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102,\n",
       "        102, 102, 102],\n",
       "       [103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103,\n",
       "        103, 103, 103],\n",
       "       [104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104,\n",
       "        104, 104, 104],\n",
       "       [105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105,\n",
       "        105, 105, 105],\n",
       "       [106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106,\n",
       "        106, 106, 106],\n",
       "       [107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107,\n",
       "        107, 107, 107],\n",
       "       [108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108,\n",
       "        108, 108, 108],\n",
       "       [109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109,\n",
       "        109, 109, 109],\n",
       "       [110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110,\n",
       "        110, 110, 110],\n",
       "       [111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111, 111,\n",
       "        111, 111, 111]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class MotionVectorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, mode, keyframe_interval=10, pad_num_boxes=52):\n",
    "        self.mode = mode\n",
    "        self.keyframe_interval = keyframe_interval\n",
    "        self.pad_num_boxes = pad_num_boxes\n",
    "        data_file = os.path.join(root_dir, \"preprocessed\", mode, \"data.pkl\")\n",
    "        self.data = pickle.load(open(data_file, \"rb\"))\n",
    "               \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.mode == \"train\" or self.mode == \"val\":\n",
    "            \n",
    "            motion_vectors = self.data[idx][\"motion_vectors\"]\n",
    "            \n",
    "            frame_idx = self.data[idx][\"frame_idx_no_skip\"]\n",
    "            gt_ids = self.data[idx][\"gt_ids\"]\n",
    "            gt_boxes = self.data[idx][\"gt_boxes\"]\n",
    "            gt_ids_prev = self.data[idx][\"gt_ids_prev\"]\n",
    "            gt_boxes_prev = self.data[idx][\"gt_boxes_prev\"]\n",
    "\n",
    "            # find boxes which occured in the last frame\n",
    "            _, idx_1, idx_0 = np.intersect1d(gt_ids, gt_ids_prev, assume_unique=True, return_indices=True)\n",
    "            boxes = torch.from_numpy(gt_boxes[idx_1])\n",
    "            boxes_prev = torch.from_numpy(gt_boxes_prev[idx_0])\n",
    "            velocities = velocities_from_boxes(boxes_prev, boxes)\n",
    "            \n",
    "            # insert frame index into boxes\n",
    "            num_boxes = (boxes.shape)[0]\n",
    "            boxes_prev_tmp = torch.zeros(num_boxes, 5)\n",
    "            boxes_prev_tmp[:, 1:5] = boxes_prev\n",
    "            boxes_prev_tmp[:, 0] = torch.full((num_boxes,), frame_idx)\n",
    "            boxes_prev = boxes_prev_tmp\n",
    "            \n",
    "            # pad boxes to the same global length (for MOT17 this is 52)\n",
    "            boxes_prev_padded = torch.zeros(self.pad_num_boxes, 5)\n",
    "            boxes_prev_padded[:num_boxes, :] = boxes_prev\n",
    "            boxes_prev = boxes_prev_padded\n",
    "            \n",
    "            # similarly pad velocites\n",
    "            velocities_padded = torch.zeros(self.pad_num_boxes, 4)\n",
    "            velocities_padded[:num_boxes, :] = velocities\n",
    "            velocities = velocities_padded\n",
    "            \n",
    "            # create a mask to revert the padding at a later stage\n",
    "            num_boxes_mask = torch.zeros(self.pad_num_boxes,)\n",
    "            num_boxes_mask[0:num_boxes] = torch.ones(num_boxes,)\n",
    "            \n",
    "            return motion_vectors.float(), boxes_prev.float(), velocities.float(), num_boxes_mask.bool()\n",
    "        \n",
    "        # TODO: handle test case"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "each sample must contain\n",
    "\n",
    "train/val:\n",
    "\n",
    "x: MV_t, B_{t-1}\n",
    "y: v_t\n",
    "\n",
    "boxes must be ordered consistently since last keyframe, only boxes with same gt_ids must be considered\n",
    "\n",
    "test:\n",
    "\n",
    "x: MV_t, Bd_{0} where t = 0 is the last keyframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagation Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropagationNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PropagationNetwork, self).__init__()\n",
    "        \n",
    "        self.POOLING_SIZE = 7  # the ROIs are split into m x m regions\n",
    "        self.FIXED_BLOCKS = 1\n",
    "        \n",
    "        self.base = torchvision.models.resnet18(pretrained=True)\n",
    "        \n",
    "        # change number of input channels from 3 to 2\n",
    "        #self.base.conv1.in_channels = 2\n",
    "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "         \n",
    "        # remove fully connected and avg pool layers\n",
    "        self.base = nn.Sequential(*list(self.base.children())[:-2])\n",
    "        \n",
    "        # change stride to 1 in conv5 block\n",
    "        #self.base[5][0].conv1 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), dilation=2, padding=(1, 1), bias=False)\n",
    "        #self.base[5][0].conv2 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), dilation=2, padding=(1, 1), bias=False)\n",
    "        \n",
    "        #self.conv1 = nn.Conv2d(512, 4, kernel_size=(1, 1), stride=(1, 1))\n",
    "        self.conv1 = nn.Conv2d(512, 4*self.POOLING_SIZE*self.POOLING_SIZE, kernel_size=(1, 1), stride=(1, 1), padding=0, bias=False)\n",
    "        \n",
    "        \n",
    "        #def set_bn_fix(m):\n",
    "        #    classname = m.__class__.__name__\n",
    "        #    print(classname)\n",
    "        #    if classname.find('BatchNorm2d') != -1:\n",
    "        #        for p in m.parameters(): p.requires_grad = False\n",
    "        #\n",
    "        #self.base.apply(set_bn_fix)\n",
    "        \n",
    "        assert (0 <= self.FIXED_BLOCKS <= 4) # set this value to 0, so we can train all blocks\n",
    "        if self.FIXED_BLOCKS >= 4: # fix all blocks\n",
    "            for p in self.base[10].parameters(): p.requires_grad = False\n",
    "        if self.FIXED_BLOCKS >= 3: # fix first 3 blocks\n",
    "            for p in self.base[8].parameters(): p.requires_grad = False\n",
    "        if self.FIXED_BLOCKS >= 2: # fix first 2 blocks\n",
    "            for p in self.base[6].parameters(): p.requires_grad = False\n",
    "        if self.FIXED_BLOCKS >= 1: # fix first 1 block\n",
    "            for p in self.base[4].parameters(): p.requires_grad = False\n",
    "        \n",
    "        #print([p.requires_grad for p in self.base.parameters()])\n",
    "        \n",
    "        \n",
    "        #print(list(self.base.children())[5][0].conv1)\n",
    "        #print(list(zip(list(self.children()), [p.requires_grad for p in self.base.parameters()])))\n",
    "        \n",
    "    def forward(self, motion_vectors, boxes_prev, num_boxes_mask):\n",
    "        #print(boxes_prev)\n",
    "        #print(\"boxes_prev:\", boxes_prev.shape)\n",
    "        #print(\"motion_vectors:\", motion_vectors.shape)\n",
    "        x = self.base(motion_vectors)\n",
    "        #print(\"after ResNet18:\", x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        #print(\"after conv1\", x.shape)\n",
    "        \n",
    "        boxes_prev = self._change_box_format(boxes_prev)\n",
    "        boxes_prev = boxes_prev[num_boxes_mask]\n",
    "        boxes_prev = boxes_prev.view(-1, 5)\n",
    "        # offset frame_idx so that it corresponds to batch index\n",
    "        boxes_prev[..., :, 0] = boxes_prev[..., :, 0] - boxes_prev[..., 0, 0]\n",
    "        \n",
    "        # compute ratio of input size to size of base output\n",
    "        spatial_scale = x.shape[-1] / (motion_vectors.shape)[-1]\n",
    "        print(spatial_scale)\n",
    "        x = torchvision.ops.ps_roi_pool(x, boxes_prev, output_size=(self.POOLING_SIZE, self.POOLING_SIZE), spatial_scale=spatial_scale)\n",
    "        #print(\"after roi_pool\", x.shape)\n",
    "        velocities_pred = x.mean(-1).mean(-1)\n",
    "        #print(\"after averaging\", velocities_pred.shape)\n",
    "        \n",
    "        return velocities_pred\n",
    "    \n",
    "    \n",
    "    def _change_box_format(self, boxes):\n",
    "        \"\"\"Change format of boxes from [x, y, w, h] to [x1, y1, x2, y2].\"\"\"\n",
    "        boxes[..., 0] = boxes[..., 0]\n",
    "        boxes[..., 1] = boxes[..., 1]\n",
    "        boxes[..., 2] = boxes[..., 2]\n",
    "        boxes[..., 3] = boxes[..., 1] + boxes[..., 3]\n",
    "        boxes[..., 4] = boxes[..., 2] + boxes[..., 4]\n",
    "        return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, num_epochs=2):\n",
    "    tstart = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    pickle.dump(best_model_wts, open(\"models/best_model.pkl\", \"wb\"))\n",
    "    best_loss = 99999.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs-1))\n",
    "        \n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "\n",
    "            for step, (motion_vectors, boxes_prev, velocities, num_boxes_mask) in enumerate(dataloaders[phase]):\n",
    "                motion_vectors = motion_vectors.to(device)\n",
    "                boxes_prev = boxes_prev.to(device)\n",
    "                velocities = velocities.to(device)\n",
    "                num_boxes_mask = num_boxes_mask.to(device)\n",
    "                \n",
    "                velocities = velocities[num_boxes_mask]\n",
    "                velocities = velocities.view(-1, 4)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    velocities_pred = model(motion_vectors, boxes_prev, num_boxes_mask)\n",
    "                    \n",
    "                    loss = criterion(velocities_pred, velocities)\n",
    "                    \n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                       \n",
    "                running_loss += loss.item() * motion_vectors.size(0)\n",
    "                \n",
    "            epoch_loss = running_loss / len(datasets[phase])\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            \n",
    "            if phase == \"val\":\n",
    "                model_wts = copy.deepcopy(model.state_dict())\n",
    "                pickle.dump(best_model_wts, open(\"models/model_{:04d}.pkl\".format(epoch), \"wb\"))\n",
    "            \n",
    "            if phase == \"val\" and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                pickle.dump(best_model_wts, open(\"models/best_model.pkl\", \"wb\"))\n",
    "                \n",
    "    time_elapsed = time.time() - tstart\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Lowest validation loss: {:4f}'.format(best_loss))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 1/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 2/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 3/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 4/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 5/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 6/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 7/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 8/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 9/59\n"
     ]
    }
   ],
   "source": [
    "datasets = {x: MotionVectorDataset(root_dir='../benchmark/MOT17', mode=x) for x in [\"train\", \"val\", \"test\"]}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=2, shuffle=False, num_workers=4) for x in [\"train\", \"val\", \"test\"]}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PropagationNetwork()\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.SmoothL1Loss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)\n",
    "best_model = train(model, criterion, optimizer, scheduler, num_epochs=60)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# test cases for utility functions\n",
    "gt_boxes_prev = np.array([[1338.,  418.,  167.,  379.],\n",
    "          [ 586.,  447.,   85.,  263.],\n",
    "          [1416.,  431.,  184.,  336.],\n",
    "          [1056.,  484.,   36.,  110.],\n",
    "          [1091.,  484.,   31.,  115.],\n",
    "          [1255.,  447.,   33.,  100.],\n",
    "          [1016.,  430.,   40.,  116.],\n",
    "          [1101.,  441.,   38.,  108.],\n",
    "          [ 935.,  436.,   42.,  114.],\n",
    "          [ 442.,  446.,  105.,  283.],\n",
    "          [ 636.,  458.,   61.,  187.],\n",
    "          [1364.,  434.,   51.,  124.],\n",
    "          [1478.,  434.,   63.,  124.],\n",
    "          [ 473.,  460.,   89.,  249.],\n",
    "          [ 548.,  465.,   35.,   93.],\n",
    "          [ 418.,  459.,   40.,   84.],\n",
    "          [ 582.,  456.,   35.,  133.],\n",
    "          [ 972.,  456.,   32.,   77.],\n",
    "          [ 578.,  432.,   20.,   43.],\n",
    "          [ 596.,  429.,   18.,   42.],\n",
    "          [ 663.,  451.,   34.,   86.]])\n",
    "\n",
    "gt_boxes = np.array([[1342.,  417.,  168.,  380.],\n",
    "          [ 586.,  446.,   85.,  264.],\n",
    "          [1422.,  431.,  183.,  337.],\n",
    "          [1055.,  483.,   36.,  110.],\n",
    "          [1090.,  484.,   32.,  114.],\n",
    "          [1255.,  447.,   33.,  100.],\n",
    "          [1015.,  430.,   40.,  116.],\n",
    "          [1100.,  440.,   38.,  108.],\n",
    "          [ 934.,  435.,   42.,  114.],\n",
    "          [ 442.,  446.,  107.,  282.],\n",
    "          [ 636.,  458.,   61.,  187.],\n",
    "          [1365.,  434.,   52.,  124.],\n",
    "          [1480.,  433.,   62.,  125.],\n",
    "          [ 473.,  460.,   89.,  249.],\n",
    "          [ 547.,  464.,   35.,   93.],\n",
    "          [ 418.,  459.,   40.,   84.],\n",
    "          [ 582.,  455.,   34.,  134.],\n",
    "          [ 972.,  456.,   32.,   77.],\n",
    "          [ 578.,  431.,   20.,   43.],\n",
    "          [ 595.,  428.,   18.,   42.],\n",
    "          [1035.,  452.,   25.,   67.],\n",
    "          [ 664.,  451.,   34.,   85.]])\n",
    "\n",
    "gt_ids = np.array([ 2.,  3.,  8.,  9., 10., 14., 15., 17., 18., 19., 20., 21., 22., 23.,\n",
    "          26., 31., 36., 39., 68., 69., 70., 72.])\n",
    "\n",
    "gt_ids_prev = np.array([ 2.,  3.,  8.,  9., 10., 14., 15., 17., 18., 19., 20., 21., 22., 23.,\n",
    "          26., 31., 36., 39., 68., 69., 72.])\n",
    "\n",
    "_, idx_1, idx_0 = np.intersect1d(gt_ids, gt_ids_prev, assume_unique=True, return_indices=True)\n",
    "print(idx_1, idx_0)\n",
    "boxes = torch.from_numpy(gt_boxes[idx_1]).unsqueeze(0)\n",
    "boxes = torch.cat([boxes, boxes], axis=0)\n",
    "boxes_prev = torch.from_numpy(gt_boxes_prev[idx_0]).unsqueeze(0)\n",
    "boxes_prev = torch.cat([boxes_prev, boxes_prev], axis=0)\n",
    "print(boxes.shape)\n",
    "print(boxes_prev.shape)\n",
    "\n",
    "velocities = velocities_from_boxes(boxes_prev, boxes)\n",
    "print(velocities)\n",
    "print(velocities.shape)\n",
    "\n",
    "\n",
    "box = box_from_velocities(boxes_prev, velocities)\n",
    "print(box)\n",
    "print(box.shape)\n",
    "print(boxes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mvs_x = (item[\"x\"].numpy())[:, :, 0]\n",
    "mvs_y = (item[\"x\"].numpy())[:, :, 1]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(25,15))\n",
    "ax.quiver(xi, yi, mvs_x, mvs_y, scale=1000, color='r')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input:\n",
    "    - motion vectors at frame t-w, t-w+1, t-w+2,... t-1, t (window length w)\n",
    "    - bounding box coordinates (x, y, w, h) at frame t-w, t-w+1, t-w+2,... t-1\n",
    "    \n",
    "Output:\n",
    "    - bounding box coordinates for frame t\n",
    "    \n",
    "Difficulties:\n",
    "    - keyframes do not have any MVs\n",
    "    - number of bounding boxes in frame can vary over time\n",
    "    - number of vectors inside bounding box can vary over time\n",
    "    - size of bounding box changes over time\n",
    "    \n",
    "Ideas:\n",
    "    1) Encode each vector as magnitude and direction\n",
    "    2) Extrapolate vector values on evenly spaced 16 x 16 grid\n",
    "    3) Use CNN-LSTM to encode the sequence of vectors and use attention to guide to bounding box region only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1080, 1920])\n",
      "torch.Size([1, 512, 34, 60])\n"
     ]
    }
   ],
   "source": [
    "net = torchvision.models.resnet18(pretrained=True)\n",
    "net = nn.Sequential(*list(net.children())[:-2])\n",
    "\n",
    "image = torch.zeros(1, 3, 1080, 1920)\n",
    "#image = torch.zeros(1, 3, 68, 120)\n",
    "output = net(image)\n",
    "print(image.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
