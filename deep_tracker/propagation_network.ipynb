{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def velocities_from_boxes(boxes_prev, boxes):\n",
    "    \"\"\"Computes bounding box velocities.\n",
    "    \n",
    "    Args:\n",
    "        boxes_prev (`torch.Tensor`): Bounding boxes in previous frame. Shape [B, N, 4]\n",
    "            where B is the batch size and N the number of bounding boxes.\n",
    "            \n",
    "        boxes (`torch.Tensor`): Bounding boxes in current frame. Shape [B, N, 4]\n",
    "            where B is the batch size and N the number of bounding boxes.\n",
    "            \n",
    "    Returns:\n",
    "        (`torch.Tensor`) velocities of box coordinates in current frame. Shape [B, N, 4].\n",
    "        \n",
    "    Ensure that ordering of boxes in both tensors is consistent and that the number of boxes\n",
    "    is the same.\n",
    "    \"\"\"\n",
    "    x = boxes[:, 0]\n",
    "    y = boxes[:, 1]\n",
    "    w = boxes[:, 2]\n",
    "    h = boxes[:, 3]\n",
    "    x_p = boxes_prev[:, 0]\n",
    "    y_p = boxes_prev[:, 1]\n",
    "    w_p = boxes_prev[:, 2]\n",
    "    h_p = boxes_prev[:, 3] \n",
    "    v_x = (1 / w_p * (x - x_p)).unsqueeze(-1)\n",
    "    v_y = (1 / h_p * (y - y_p)).unsqueeze(-1)\n",
    "    v_w = (torch.log(w / w_p)).unsqueeze(-1)\n",
    "    v_h = (torch.log(h / h_p)).unsqueeze(-1) \n",
    "    return torch.cat([v_x, v_y, v_w, v_h], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_from_velocities(boxes_prev, velocities):\n",
    "    \"\"\"Computes bounding boxes from previous boxes and velocities.\n",
    "    \n",
    "    Args:\n",
    "        boxes_prev (`torch.Tensor`): Bounding boxes in previous frame. Shape [B, N, 4] \n",
    "            where B is the batch size and N the number of bounding boxes.\n",
    "            \n",
    "        velocities (`torch.Tensor`): Box velocities in current frame. Shape [B, N, 4] \n",
    "        where B is the batch size and N the number of bounding boxes.\n",
    "            \n",
    "    Returns:\n",
    "        (`torch.Tensor`) Bounding boxes in current frame. Shape [B, N, 4].\n",
    "        \n",
    "    Ensure that ordering of boxes and velocities in both tensors is consistent that is\n",
    "    box in row i should correspond to velocities in row i.\n",
    "    \"\"\"\n",
    "    x_p = boxes_prev[:, 0]\n",
    "    y_p = boxes_prev[:, 1]\n",
    "    w_p = boxes_prev[:, 2]\n",
    "    h_p = boxes_prev[:, 3]\n",
    "    v_x = velocities[:, 0]\n",
    "    v_y = velocities[:, 1]\n",
    "    v_w = velocities[:, 2]\n",
    "    v_h = velocities[:, 3]\n",
    "    x = (w_p * v_x + x_p).unsqueeze(-1)\n",
    "    y = (h_p * v_y + y_p).unsqueeze(-1)\n",
    "    w = (w_p * torch.exp(v_w)).unsqueeze(-1)\n",
    "    h = (h_p * torch.exp(v_h)).unsqueeze(-1)\n",
    "    return torch.cat([x, y, w, h], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotionVectorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, mode, keyframe_interval=10, pad_num_boxes=52):\n",
    "        self.mode = mode\n",
    "        self.keyframe_interval = keyframe_interval\n",
    "        self.pad_num_boxes = pad_num_boxes\n",
    "        data_file = os.path.join(root_dir, \"preprocessed\", mode, \"data.pkl\")\n",
    "        self.data = pickle.load(open(data_file, \"rb\"))\n",
    "               \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.mode == \"train\" or self.mode == \"val\":\n",
    "            \n",
    "            motion_vectors = self.data[idx][\"motion_vectors\"]\n",
    "            \n",
    "            frame_idx = self.data[idx][\"frame_idx_no_skip\"]\n",
    "            gt_ids = self.data[idx][\"gt_ids\"]\n",
    "            gt_boxes = self.data[idx][\"gt_boxes\"]\n",
    "            gt_ids_prev = self.data[idx][\"gt_ids_prev\"]\n",
    "            gt_boxes_prev = self.data[idx][\"gt_boxes_prev\"]\n",
    "\n",
    "            # find boxes which occured in the last frame\n",
    "            _, idx_1, idx_0 = np.intersect1d(gt_ids, gt_ids_prev, assume_unique=True, return_indices=True)\n",
    "            boxes = torch.from_numpy(gt_boxes[idx_1])\n",
    "            boxes_prev = torch.from_numpy(gt_boxes_prev[idx_0])\n",
    "            velocities = velocities_from_boxes(boxes_prev, boxes)\n",
    "            \n",
    "            # insert frame index into boxes\n",
    "            num_boxes = (boxes.shape)[0]\n",
    "            boxes_prev_tmp = torch.zeros(num_boxes, 5)\n",
    "            boxes_prev_tmp[:, 1:5] = boxes_prev\n",
    "            boxes_prev_tmp[:, 0] = torch.full((num_boxes,), frame_idx)\n",
    "            boxes_prev = boxes_prev_tmp\n",
    "            \n",
    "            # pad boxes to the same global length (for MOT17 this is 52)\n",
    "            boxes_prev_padded = torch.zeros(self.pad_num_boxes, 5)\n",
    "            boxes_prev_padded[:num_boxes, :] = boxes_prev\n",
    "            boxes_prev = boxes_prev_padded\n",
    "            \n",
    "            # similarly pad velocites\n",
    "            velocities_padded = torch.zeros(self.pad_num_boxes, 4)\n",
    "            velocities_padded[:num_boxes, :] = velocities\n",
    "            velocities = velocities_padded\n",
    "            \n",
    "            # create a mask to revert the padding at a later stage\n",
    "            num_boxes_mask = torch.zeros(self.pad_num_boxes,)\n",
    "            num_boxes_mask[0:num_boxes] = torch.ones(num_boxes,)\n",
    "            \n",
    "            return motion_vectors.float(), boxes_prev.float(), velocities.float(), num_boxes_mask.bool()\n",
    "        \n",
    "        # TODO: handle test case"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "each sample must contain\n",
    "\n",
    "train/val:\n",
    "\n",
    "x: MV_t, B_{t-1}\n",
    "y: v_t\n",
    "\n",
    "boxes must be ordered consistently since last keyframe, only boxes with same gt_ids must be considered\n",
    "\n",
    "test:\n",
    "\n",
    "x: MV_t, Bd_{0} where t = 0 is the last keyframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagation Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropagationNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PropagationNetwork, self).__init__()\n",
    "        \n",
    "        self.m = 7  # the ROIs are split into m x m regions\n",
    "        self.base = torchvision.models.resnet18(pretrained=True)\n",
    "        \n",
    "        # change number of input channels from 3 to 2\n",
    "        #self.base.conv1.in_channels = 2\n",
    "        self.base.conv1 = nn.Conv2d(2, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "         \n",
    "        # remove fully connected and avg pool layers\n",
    "        self.base = nn.Sequential(*list(self.base.children())[:-2])\n",
    "        \n",
    "        # change stride to 1 in conv5 block\n",
    "        #self.base[5][0].conv1 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), dilation=2, padding=(1, 1), bias=False)\n",
    "        #self.base[5][0].conv2 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), dilation=2, padding=(1, 1), bias=False)\n",
    "        \n",
    "        #self.conv1 = nn.Conv2d(512, 4, kernel_size=(1, 1), stride=(1, 1))\n",
    "        self.conv1 = nn.Conv2d(512, 4*self.m*self.m, kernel_size=(1, 1), stride=(1, 1))\n",
    "        \n",
    "        #print(list(self.base.children())[5][0].conv1)\n",
    "        #print(list(self.children()))\n",
    "        \n",
    "    def forward(self, motion_vectors, boxes_prev, num_boxes_mask):\n",
    "        #print(boxes_prev)\n",
    "        #print(\"boxes_prev:\", boxes_prev.shape)\n",
    "        #print(\"motion_vectors:\", motion_vectors.shape)\n",
    "        x = self.base(motion_vectors)\n",
    "        #print(\"after ResNet18:\", x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        #print(\"after conv1\", x.shape)\n",
    "        \n",
    "        boxes_prev = self._change_box_format(boxes_prev)\n",
    "        boxes_prev = boxes_prev[num_boxes_mask]\n",
    "        boxes_prev = boxes_prev.view(-1, 5)\n",
    "        # offset frame_idx so that it corresponds to batch index\n",
    "        boxes_prev[..., :, 0] = boxes_prev[..., :, 0] - boxes_prev[..., 0, 0]\n",
    "        \n",
    "        # compute ratio of input size to size of base output\n",
    "        spatial_scale = x.shape[-1] / (motion_vectors.shape)[-1]\n",
    "        x = ops.ps_roi_pool(x, boxes_prev, output_size=(self.m, self.m), spatial_scale=spatial_scale)\n",
    "        #print(\"after roi_pool\", x.shape)\n",
    "        velocities_pred = x.mean(-1).mean(-1)\n",
    "        #print(\"after averaging\", velocities_pred.shape)\n",
    "        \n",
    "        return velocities_pred\n",
    "    \n",
    "    \n",
    "    def _change_box_format(self, boxes):\n",
    "        \"\"\"Change format of boxes from [x, y, w, h] to [x1, y1, x2, y2].\"\"\"\n",
    "        boxes[..., 0] = boxes[..., 0]\n",
    "        boxes[..., 1] = boxes[..., 1]\n",
    "        boxes[..., 2] = boxes[..., 2]\n",
    "        boxes[..., 3] = boxes[..., 1] + boxes[..., 3]\n",
    "        boxes[..., 4] = boxes[..., 2] + boxes[..., 4]\n",
    "        return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, scheduler, num_epochs=2):\n",
    "    tstart = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    pickle.dump(best_model_wts, open(\"models/best_model.pkl\", \"wb\"))\n",
    "    best_loss = 99999.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs-1))\n",
    "        \n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "                \n",
    "            running_loss = 0.0\n",
    "\n",
    "            for step, (motion_vectors, boxes_prev, velocities, num_boxes_mask) in enumerate(dataloaders[phase]):\n",
    "                motion_vectors = motion_vectors.to(device)\n",
    "                boxes_prev = boxes_prev.to(device)\n",
    "                velocities = velocities.to(device)\n",
    "                num_boxes_mask = num_boxes_mask.to(device)\n",
    "                \n",
    "                velocities = velocities[num_boxes_mask]\n",
    "                velocities = velocities.view(-1, 4)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    velocities_pred = model(motion_vectors, boxes_prev, num_boxes_mask)\n",
    "                    \n",
    "                    loss = criterion(velocities_pred, velocities)\n",
    "                    \n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                       \n",
    "                running_loss += loss.item() * motion_vectors.size(0)\n",
    "                \n",
    "            epoch_loss = running_loss / len(datasets[phase])\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            \n",
    "            if phase == \"val\":\n",
    "                model_wts = copy.deepcopy(model.state_dict())\n",
    "                pickle.dump(best_model_wts, open(\"models/model_{:04d}.pkl\".format(epoch), \"wb\"))\n",
    "            \n",
    "            if phase == \"val\" and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                pickle.dump(best_model_wts, open(\"models/best_model.pkl\", \"wb\"))\n",
    "                \n",
    "    time_elapsed = time.time() - tstart\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Lowest validation loss: {:4f}'.format(best_loss))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 1/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 2/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 3/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 4/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 5/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 6/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 7/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 8/59\n",
      "train Loss: 0.0007\n",
      "val Loss: 0.0015\n",
      "Epoch 9/59\n"
     ]
    }
   ],
   "source": [
    "datasets = {x: MotionVectorDataset(root_dir='../benchmark/MOT17', mode=x) for x in [\"train\", \"val\", \"test\"]}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=2, shuffle=False, num_workers=4) for x in [\"train\", \"val\", \"test\"]}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PropagationNetwork()\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.SmoothL1Loss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)\n",
    "best_model = train(model, criterion, optimizer, scheduler, num_epochs=60)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# test cases for utility functions\n",
    "gt_boxes_prev = np.array([[1338.,  418.,  167.,  379.],\n",
    "          [ 586.,  447.,   85.,  263.],\n",
    "          [1416.,  431.,  184.,  336.],\n",
    "          [1056.,  484.,   36.,  110.],\n",
    "          [1091.,  484.,   31.,  115.],\n",
    "          [1255.,  447.,   33.,  100.],\n",
    "          [1016.,  430.,   40.,  116.],\n",
    "          [1101.,  441.,   38.,  108.],\n",
    "          [ 935.,  436.,   42.,  114.],\n",
    "          [ 442.,  446.,  105.,  283.],\n",
    "          [ 636.,  458.,   61.,  187.],\n",
    "          [1364.,  434.,   51.,  124.],\n",
    "          [1478.,  434.,   63.,  124.],\n",
    "          [ 473.,  460.,   89.,  249.],\n",
    "          [ 548.,  465.,   35.,   93.],\n",
    "          [ 418.,  459.,   40.,   84.],\n",
    "          [ 582.,  456.,   35.,  133.],\n",
    "          [ 972.,  456.,   32.,   77.],\n",
    "          [ 578.,  432.,   20.,   43.],\n",
    "          [ 596.,  429.,   18.,   42.],\n",
    "          [ 663.,  451.,   34.,   86.]])\n",
    "\n",
    "gt_boxes = np.array([[1342.,  417.,  168.,  380.],\n",
    "          [ 586.,  446.,   85.,  264.],\n",
    "          [1422.,  431.,  183.,  337.],\n",
    "          [1055.,  483.,   36.,  110.],\n",
    "          [1090.,  484.,   32.,  114.],\n",
    "          [1255.,  447.,   33.,  100.],\n",
    "          [1015.,  430.,   40.,  116.],\n",
    "          [1100.,  440.,   38.,  108.],\n",
    "          [ 934.,  435.,   42.,  114.],\n",
    "          [ 442.,  446.,  107.,  282.],\n",
    "          [ 636.,  458.,   61.,  187.],\n",
    "          [1365.,  434.,   52.,  124.],\n",
    "          [1480.,  433.,   62.,  125.],\n",
    "          [ 473.,  460.,   89.,  249.],\n",
    "          [ 547.,  464.,   35.,   93.],\n",
    "          [ 418.,  459.,   40.,   84.],\n",
    "          [ 582.,  455.,   34.,  134.],\n",
    "          [ 972.,  456.,   32.,   77.],\n",
    "          [ 578.,  431.,   20.,   43.],\n",
    "          [ 595.,  428.,   18.,   42.],\n",
    "          [1035.,  452.,   25.,   67.],\n",
    "          [ 664.,  451.,   34.,   85.]])\n",
    "\n",
    "gt_ids = np.array([ 2.,  3.,  8.,  9., 10., 14., 15., 17., 18., 19., 20., 21., 22., 23.,\n",
    "          26., 31., 36., 39., 68., 69., 70., 72.])\n",
    "\n",
    "gt_ids_prev = np.array([ 2.,  3.,  8.,  9., 10., 14., 15., 17., 18., 19., 20., 21., 22., 23.,\n",
    "          26., 31., 36., 39., 68., 69., 72.])\n",
    "\n",
    "_, idx_1, idx_0 = np.intersect1d(gt_ids, gt_ids_prev, assume_unique=True, return_indices=True)\n",
    "print(idx_1, idx_0)\n",
    "boxes = torch.from_numpy(gt_boxes[idx_1]).unsqueeze(0)\n",
    "boxes = torch.cat([boxes, boxes], axis=0)\n",
    "boxes_prev = torch.from_numpy(gt_boxes_prev[idx_0]).unsqueeze(0)\n",
    "boxes_prev = torch.cat([boxes_prev, boxes_prev], axis=0)\n",
    "print(boxes.shape)\n",
    "print(boxes_prev.shape)\n",
    "\n",
    "velocities = velocities_from_boxes(boxes_prev, boxes)\n",
    "print(velocities)\n",
    "print(velocities.shape)\n",
    "\n",
    "\n",
    "box = box_from_velocities(boxes_prev, velocities)\n",
    "print(box)\n",
    "print(box.shape)\n",
    "print(boxes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mvs_x = (item[\"x\"].numpy())[:, :, 0]\n",
    "mvs_y = (item[\"x\"].numpy())[:, :, 1]\n",
    "\n",
    "f, ax = plt.subplots(figsize=(25,15))\n",
    "ax.quiver(xi, yi, mvs_x, mvs_y, scale=1000, color='r')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input:\n",
    "    - motion vectors at frame t-w, t-w+1, t-w+2,... t-1, t (window length w)\n",
    "    - bounding box coordinates (x, y, w, h) at frame t-w, t-w+1, t-w+2,... t-1\n",
    "    \n",
    "Output:\n",
    "    - bounding box coordinates for frame t\n",
    "    \n",
    "Difficulties:\n",
    "    - keyframes do not have any MVs\n",
    "    - number of bounding boxes in frame can vary over time\n",
    "    - number of vectors inside bounding box can vary over time\n",
    "    - size of bounding box changes over time\n",
    "    \n",
    "Ideas:\n",
    "    1) Encode each vector as magnitude and direction\n",
    "    2) Extrapolate vector values on evenly spaced 16 x 16 grid\n",
    "    3) Use CNN-LSTM to encode the sequence of vectors and use attention to guide to bounding box region only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
